{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXTRACTION OF DATA FROM XML FILES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\vishn\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# EXTRACTION OF DATA FROM XML FILE\n",
    "import glob\n",
    "import os\n",
    "\n",
    "import numpy\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from lxml import etree\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, BaggingClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_auc_score, davies_bouldin_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import StandardScaler, LabelBinarizer, LabelEncoder\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn import metrics\n",
    "from keras.layers import Input, Dense, Dropout\n",
    "from keras.models import Model\n",
    "from gensim.models import word2vec\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from sklearn import preprocessing\n",
    "from keras import Sequential\n",
    "\n",
    "\n",
    "dataColumns = [\"headline\", \"text\", \"bip:topics\", \"dc.date.published\", \"itemID\", \"XML_File_Name\"]\n",
    "clusterDataframeList = []\n",
    "rows = []\n",
    "paragraph = \"\"\n",
    "bipTopicList = []\n",
    "vec = CountVectorizer(stop_words=None)\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_features=10000, max_df=0.5, use_idf=True)\n",
    "enhancedDFList = []\n",
    "\n",
    "\n",
    "def dataExtraction():\n",
    "    dir = '/Users/vishn/Data'\n",
    "    for file in glob.iglob(os.path.join(dir, '*.xml')):\n",
    "        paragraph = \"\"\n",
    "        bipTopicCode = \"\"\n",
    "        path, fileName = os.path.split(file)  # Obtained File name\n",
    "        data = etree.parse(file)\n",
    "        root = data.getroot()\n",
    "        itemId = data.getroot().attrib.get(\"itemid\")  # Obtained item ID\n",
    "        headline = data.find(\"headline\").text\n",
    "        textNode = data.find(\"text\")\n",
    "        for node in textNode:\n",
    "            paragraph = paragraph + node.text  # Obtained text\n",
    "        dcPublishedNode = root.findall(\"./metadata/dc[@element='dc.date.published']\")\n",
    "        if dcPublishedNode is not None:\n",
    "            published_date = dcPublishedNode[0].attrib.get(\"value\")  # obtained dc.date.published\n",
    "        else:\n",
    "            published_date = \"NONE\"\n",
    "        bipNode = root.findall(\"./metadata/codes[@class='bip:topics:1.0']/code\")\n",
    "        text = removeStopWords(paragraph)  # removing stop words\n",
    "        if bipNode is not None:\n",
    "            for innercodes in bipNode:\n",
    "                bipTopicCode = innercodes.attrib.get(\"code\")  # obtained bip:topic code\n",
    "                rows.append({\"itemID\": itemId, \"XML_File_Name\": fileName, \"headline\": headline, \"text\": text,\n",
    "                             \"dc.date.published\": published_date, \"bip:topics\": bipTopicCode})\n",
    "                uniqueBipTopics(bipTopicCode)\n",
    "                break\n",
    "        else:\n",
    "            bipTopicCode = \"NONE\"\n",
    "            rows.append({\"itemID\": itemId, \"XML_File_Name\": fileName, \"headline\": headline, \"text\": text,\n",
    "                         \"dc.date.published\": published_date, \"bip:topics\": bipTopicCode})\n",
    "\n",
    "    customDataFrame = pd.DataFrame(rows, columns=dataColumns)\n",
    "    return customDataFrame\n",
    "\n",
    "\n",
    "def uniqueBipTopics(topic):\n",
    "    if topic not in bipTopicList:\n",
    "        bipTopicList.append(topic)\n",
    "    return bipTopicList\n",
    "\n",
    "\n",
    "def removeStopWords(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    text_tokens = word_tokenize(text)\n",
    "    filtered_sentence_list = [w for w in text_tokens if w not in stop_words]\n",
    "    filtered_lemmatized_list = lemmatization(filtered_sentence_list)\n",
    "    filtered_stemmed_list = stemming(filtered_lemmatized_list)\n",
    "    filtered_lemmatized_sentence = ' '.join(filtered_stemmed_list)\n",
    "    return filtered_lemmatized_sentence\n",
    "\n",
    "\n",
    "def stemming(sentence):\n",
    "    ps = PorterStemmer()\n",
    "    stemmed_words = []\n",
    "    for w in sentence:\n",
    "        stemmed_words.append(ps.stem(w))\n",
    "    return stemmed_words\n",
    "\n",
    "\n",
    "def lemmatization(filtered_sentence):\n",
    "    lem = WordNetLemmatizer()\n",
    "    lemmatized_words = []\n",
    "    for w in filtered_sentence:\n",
    "        lemmatized_words.append(lem.lemmatize(w))\n",
    "    return lemmatized_words\n",
    "rawDataFrame = dataExtraction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline</th>\n",
       "      <th>text</th>\n",
       "      <th>bip:topics</th>\n",
       "      <th>dc.date.published</th>\n",
       "      <th>itemID</th>\n",
       "      <th>XML_File_Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Canadian Occidental mounts rival Wascana bid.</td>\n",
       "      <td>canadian occident petroleum ltd. emerg tuesday...</td>\n",
       "      <td>C181</td>\n",
       "      <td>1997-03-18</td>\n",
       "      <td>326914</td>\n",
       "      <td>326914newsML.xml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Gruma, Maseca to receive syndicated loan - bank.</td>\n",
       "      <td>bank america launch three-year $ 120 million s...</td>\n",
       "      <td>C173</td>\n",
       "      <td>1997-03-18</td>\n",
       "      <td>326915</td>\n",
       "      <td>326915newsML.xml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Too early to call Krupp bid hostile - Deutsche...</td>\n",
       "      <td>deutsch bank AG manag board member rolf breuer...</td>\n",
       "      <td>C18</td>\n",
       "      <td>1997-03-18</td>\n",
       "      <td>326916</td>\n",
       "      <td>326916newsML.xml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FOCUS - Euro bourses fret over Wall St, electi...</td>\n",
       "      <td>european bours fell tuesday even wall street o...</td>\n",
       "      <td>M11</td>\n",
       "      <td>1997-03-18</td>\n",
       "      <td>326917</td>\n",
       "      <td>326917newsML.xml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>French stocks fall, Alcatel posts big gain.</td>\n",
       "      <td>french share close lower tuesday second consec...</td>\n",
       "      <td>G152</td>\n",
       "      <td>1997-03-18</td>\n",
       "      <td>326918</td>\n",
       "      <td>326918newsML.xml</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            headline  \\\n",
       "0      Canadian Occidental mounts rival Wascana bid.   \n",
       "1   Gruma, Maseca to receive syndicated loan - bank.   \n",
       "2  Too early to call Krupp bid hostile - Deutsche...   \n",
       "3  FOCUS - Euro bourses fret over Wall St, electi...   \n",
       "4        French stocks fall, Alcatel posts big gain.   \n",
       "\n",
       "                                                text bip:topics  \\\n",
       "0  canadian occident petroleum ltd. emerg tuesday...       C181   \n",
       "1  bank america launch three-year $ 120 million s...       C173   \n",
       "2  deutsch bank AG manag board member rolf breuer...        C18   \n",
       "3  european bours fell tuesday even wall street o...        M11   \n",
       "4  french share close lower tuesday second consec...       G152   \n",
       "\n",
       "  dc.date.published  itemID     XML_File_Name  \n",
       "0        1997-03-18  326914  326914newsML.xml  \n",
       "1        1997-03-18  326915  326915newsML.xml  \n",
       "2        1997-03-18  326916  326916newsML.xml  \n",
       "3        1997-03-18  326917  326917newsML.xml  \n",
       "4        1997-03-18  326918  326918newsML.xml  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rawDataFrame.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CLUSTERING DOCUMENTS USING K MEANS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PERFORMING CLUSTERING ON DOCUMENTS\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "def clustering(clusterDataFrame):\n",
    "    docVecList = []\n",
    "    textData = clusterDataFrame[\"text\"]\n",
    "    bipTopics = clusterDataFrame[\"bip:topics\"]\n",
    "\n",
    "    # The goal of doc2vec is to create a numeric representation of a document\n",
    "    tagged_data = [TaggedDocument(words=word_tokenize(_d.lower()), tags=[str(i)]) for i, _d in enumerate(textData)]\n",
    "    max_epochs = 20\n",
    "    # vec_size is used to set dimension of the vector. below vec size indicates the representation\n",
    "    # of document in 20 components\n",
    "    vec_size = 20\n",
    "    alpha = 0.025 #Learning rate\n",
    "    model = Doc2Vec(size=vec_size, alpha=alpha, min_alpha=0.00025, min_count=1, dm=1)\n",
    "    #dm defines the training algorithm. If dm=1 means ‘distributed memory’ (PV-DM) and dm =0 means ‘distributed bag of words’\n",
    "    # (PV-DBOW). Distributed Memory model preserves the word order in a document whereas Distributed Bag of words just uses the bag of\n",
    "    # words approach, which doesn’t preserve any word order.\n",
    "    model.build_vocab(tagged_data)\n",
    "    for epoch in range(max_epochs):\n",
    "        model.train(tagged_data,\n",
    "                    total_examples=model.corpus_count,\n",
    "                    epochs=model.iter)\n",
    "        # decrease the learning rate\n",
    "        model.alpha -= 0.0002\n",
    "        # fix the learning rate, no decay\n",
    "        model.min_alpha = model.alpha\n",
    "    for i in range((len(tagged_data))):\n",
    "        docVecList.append(model.docvecs[i])\n",
    "\n",
    "\n",
    "    featureDataFrame = pd.DataFrame(data=docVecList)\n",
    "    min_max_scaler = preprocessing.MinMaxScaler()\n",
    "    scaledFeatureArray = min_max_scaler.fit_transform(featureDataFrame)\n",
    "    scaledFeatureDataFrame = pd.DataFrame(data=scaledFeatureArray)\n",
    "    km = KMeans(n_clusters=10) #TAKEN 10 CLUSTERS\n",
    "    km.fit(scaledFeatureDataFrame)\n",
    "    clusters = km.labels_\n",
    "    scaledFeatureDataFrame['cluster_ID'] = clusters\n",
    "    scaledFeatureDataFrame['labels'] = bipTopics\n",
    "    # clusterQuality(clusters,featureData,featureDataFrame)\n",
    "    # return FeatureDataFrame\n",
    "    return scaledFeatureDataFrame,clusters,featureDataFrame\n",
    "\n",
    "receivedClusterDataframe, receivedClusters, receivedFeatureData = clustering(rawDataFrame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>cluster_ID</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.622538</td>\n",
       "      <td>0.545301</td>\n",
       "      <td>0.752230</td>\n",
       "      <td>0.619713</td>\n",
       "      <td>0.468593</td>\n",
       "      <td>0.576692</td>\n",
       "      <td>0.548873</td>\n",
       "      <td>0.620881</td>\n",
       "      <td>0.475798</td>\n",
       "      <td>0.055148</td>\n",
       "      <td>...</td>\n",
       "      <td>0.461509</td>\n",
       "      <td>0.519127</td>\n",
       "      <td>0.273841</td>\n",
       "      <td>0.269771</td>\n",
       "      <td>0.746221</td>\n",
       "      <td>0.503637</td>\n",
       "      <td>0.479970</td>\n",
       "      <td>0.662316</td>\n",
       "      <td>2</td>\n",
       "      <td>C181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.685125</td>\n",
       "      <td>0.524099</td>\n",
       "      <td>0.719955</td>\n",
       "      <td>0.669576</td>\n",
       "      <td>0.425134</td>\n",
       "      <td>0.459874</td>\n",
       "      <td>0.579058</td>\n",
       "      <td>0.727017</td>\n",
       "      <td>0.528013</td>\n",
       "      <td>0.267745</td>\n",
       "      <td>...</td>\n",
       "      <td>0.472078</td>\n",
       "      <td>0.627598</td>\n",
       "      <td>0.290054</td>\n",
       "      <td>0.466580</td>\n",
       "      <td>0.787930</td>\n",
       "      <td>0.253786</td>\n",
       "      <td>0.502149</td>\n",
       "      <td>0.637086</td>\n",
       "      <td>2</td>\n",
       "      <td>C173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.502486</td>\n",
       "      <td>0.505510</td>\n",
       "      <td>0.686061</td>\n",
       "      <td>0.556233</td>\n",
       "      <td>0.161481</td>\n",
       "      <td>0.281391</td>\n",
       "      <td>0.133655</td>\n",
       "      <td>0.689424</td>\n",
       "      <td>0.568082</td>\n",
       "      <td>0.252426</td>\n",
       "      <td>...</td>\n",
       "      <td>0.489413</td>\n",
       "      <td>0.623307</td>\n",
       "      <td>0.288272</td>\n",
       "      <td>0.325249</td>\n",
       "      <td>0.763247</td>\n",
       "      <td>0.418129</td>\n",
       "      <td>0.672976</td>\n",
       "      <td>0.523832</td>\n",
       "      <td>9</td>\n",
       "      <td>C18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.443644</td>\n",
       "      <td>0.521659</td>\n",
       "      <td>0.757898</td>\n",
       "      <td>0.549458</td>\n",
       "      <td>0.224391</td>\n",
       "      <td>0.182809</td>\n",
       "      <td>0.311964</td>\n",
       "      <td>0.773963</td>\n",
       "      <td>0.556796</td>\n",
       "      <td>0.278611</td>\n",
       "      <td>...</td>\n",
       "      <td>0.460008</td>\n",
       "      <td>0.650015</td>\n",
       "      <td>0.460365</td>\n",
       "      <td>0.419992</td>\n",
       "      <td>0.722694</td>\n",
       "      <td>0.489154</td>\n",
       "      <td>0.746901</td>\n",
       "      <td>0.500162</td>\n",
       "      <td>4</td>\n",
       "      <td>M11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.409186</td>\n",
       "      <td>0.572101</td>\n",
       "      <td>0.740241</td>\n",
       "      <td>0.530068</td>\n",
       "      <td>0.213700</td>\n",
       "      <td>0.212124</td>\n",
       "      <td>0.312620</td>\n",
       "      <td>0.744341</td>\n",
       "      <td>0.420685</td>\n",
       "      <td>0.306090</td>\n",
       "      <td>...</td>\n",
       "      <td>0.460553</td>\n",
       "      <td>0.580583</td>\n",
       "      <td>0.396706</td>\n",
       "      <td>0.394423</td>\n",
       "      <td>0.767854</td>\n",
       "      <td>0.383323</td>\n",
       "      <td>0.809262</td>\n",
       "      <td>0.548098</td>\n",
       "      <td>4</td>\n",
       "      <td>G152</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6  \\\n",
       "0  0.622538  0.545301  0.752230  0.619713  0.468593  0.576692  0.548873   \n",
       "1  0.685125  0.524099  0.719955  0.669576  0.425134  0.459874  0.579058   \n",
       "2  0.502486  0.505510  0.686061  0.556233  0.161481  0.281391  0.133655   \n",
       "3  0.443644  0.521659  0.757898  0.549458  0.224391  0.182809  0.311964   \n",
       "4  0.409186  0.572101  0.740241  0.530068  0.213700  0.212124  0.312620   \n",
       "\n",
       "          7         8         9  ...        12        13        14        15  \\\n",
       "0  0.620881  0.475798  0.055148  ...  0.461509  0.519127  0.273841  0.269771   \n",
       "1  0.727017  0.528013  0.267745  ...  0.472078  0.627598  0.290054  0.466580   \n",
       "2  0.689424  0.568082  0.252426  ...  0.489413  0.623307  0.288272  0.325249   \n",
       "3  0.773963  0.556796  0.278611  ...  0.460008  0.650015  0.460365  0.419992   \n",
       "4  0.744341  0.420685  0.306090  ...  0.460553  0.580583  0.396706  0.394423   \n",
       "\n",
       "         16        17        18        19  cluster_ID  labels  \n",
       "0  0.746221  0.503637  0.479970  0.662316           2    C181  \n",
       "1  0.787930  0.253786  0.502149  0.637086           2    C173  \n",
       "2  0.763247  0.418129  0.672976  0.523832           9     C18  \n",
       "3  0.722694  0.489154  0.746901  0.500162           4     M11  \n",
       "4  0.767854  0.383323  0.809262  0.548098           4    G152  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "receivedClusterDataframe.head() # PRINING THE DATAFRAME AFTER CLUSTERING WITH cluster_id INCLUDED AS COLUMN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DIVIDING DATA ACCORDING TO CLUSTERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DIVIDING THE DATAFRAME ACCORDING TO  CLUSTERS\n",
    "def clusterProcessing(receivedClusterDataframe):\n",
    "    uniqueClusterIDs = receivedClusterDataframe.cluster_ID.unique()\n",
    "    uniqueClusterIDs.sort()\n",
    "    for id in uniqueClusterIDs:\n",
    "        clusterWiseDF = receivedClusterDataframe.loc[receivedClusterDataframe['cluster_ID'] == id]\n",
    "        clusterDataframeList.append(clusterWiseDF)\n",
    "    return clusterDataframeList\n",
    "\n",
    "clusterDataframeList = clusterProcessing(receivedClusterDataframe)\n",
    "frameListforEnhance = clusterDataframeList[:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[              0         1         2         3         4         5         6  \\\n",
       " 28     0.622508  0.316628  0.742297  0.663221  0.350849  0.173662  0.438305   \n",
       " 29     0.541481  0.410301  0.758444  0.614542  0.389700  0.267358  0.412344   \n",
       " 30     0.629755  0.489406  0.746179  0.636506  0.390949  0.293430  0.343126   \n",
       " 34     0.490809  0.393096  0.734348  0.496565  0.293426  0.128871  0.521706   \n",
       " 37     0.586118  0.328403  0.721332  0.574498  0.310174  0.142900  0.366615   \n",
       " ...         ...       ...       ...       ...       ...       ...       ...   \n",
       " 48235  0.581130  0.312766  0.615109  0.562801  0.222225  0.245799  0.508059   \n",
       " 48236  0.552760  0.288122  0.632354  0.548497  0.209318  0.244996  0.516977   \n",
       " 48237  0.453025  0.361840  0.805629  0.659009  0.303432  0.316699  0.572791   \n",
       " 48238  0.605836  0.360852  0.682861  0.726450  0.387849  0.218006  0.417745   \n",
       " 48250  0.604980  0.426017  0.709831  0.677164  0.303783  0.216087  0.514777   \n",
       " \n",
       "               7         8         9  ...        12        13        14  \\\n",
       " 28     0.498225  0.528881  0.325895  ...  0.441763  0.556931  0.482738   \n",
       " 29     0.572790  0.423142  0.321519  ...  0.507642  0.699339  0.455692   \n",
       " 30     0.520117  0.488679  0.299681  ...  0.472562  0.478311  0.351168   \n",
       " 34     0.631890  0.592417  0.298124  ...  0.471264  0.597279  0.413895   \n",
       " 37     0.480230  0.503807  0.309011  ...  0.550417  0.484542  0.432625   \n",
       " ...         ...       ...       ...  ...       ...       ...       ...   \n",
       " 48235  0.549666  0.666243  0.383311  ...  0.479449  0.588689  0.683952   \n",
       " 48236  0.549631  0.687082  0.388547  ...  0.463943  0.587971  0.692615   \n",
       " 48237  0.566424  0.650281  0.404194  ...  0.612309  0.648036  0.407045   \n",
       " 48238  0.500088  0.406569  0.333624  ...  0.448739  0.595201  0.523815   \n",
       " 48250  0.533132  0.534119  0.318485  ...  0.603183  0.666343  0.265567   \n",
       " \n",
       "              15        16        17        18        19  cluster_ID  labels  \n",
       " 28     0.421832  0.827740  0.521070  0.613639  0.548572           0     C13  \n",
       " 29     0.451479  0.842808  0.473079  0.685691  0.488308           0     C13  \n",
       " 30     0.532923  0.845322  0.556701  0.694781  0.602580           0     C13  \n",
       " 34     0.375817  0.760801  0.409776  0.697465  0.437017           0     E11  \n",
       " 37     0.510414  0.852415  0.420414  0.533012  0.528438           0     C24  \n",
       " ...         ...       ...       ...       ...       ...         ...     ...  \n",
       " 48235  0.465487  0.658946  0.665690  0.636236  0.647739           0     M12  \n",
       " 48236  0.481869  0.651839  0.702034  0.621026  0.668700           0     M12  \n",
       " 48237  0.368168  0.807037  0.411538  0.695743  0.525483           0    GCAT  \n",
       " 48238  0.443795  0.832485  0.464737  0.639970  0.507788           0    GCAT  \n",
       " 48250  0.526547  0.740273  0.417593  0.775542  0.607195           0    C183  \n",
       " \n",
       " [4415 rows x 22 columns],\n",
       "               0         1         2         3         4         5         6  \\\n",
       " 18     0.556025  0.589608  0.756549  0.556947  0.418999  0.359088  0.501323   \n",
       " 21     0.329707  0.434719  0.919971  0.570100  0.476718  0.214956  0.613630   \n",
       " 49     0.533088  0.238573  0.729541  0.600084  0.458003  0.245194  0.475470   \n",
       " 55     0.598694  0.446052  0.632625  0.582317  0.331320  0.430070  0.479354   \n",
       " 64     0.590582  0.488045  0.613023  0.617731  0.297865  0.432904  0.477711   \n",
       " ...         ...       ...       ...       ...       ...       ...       ...   \n",
       " 48239  0.606009  0.450273  0.721721  0.690324  0.310727  0.261675  0.392899   \n",
       " 48240  0.628819  0.324015  0.669734  0.623548  0.220870  0.427986  0.609176   \n",
       " 48245  0.571268  0.385716  0.856120  0.703906  0.470930  0.361529  0.531860   \n",
       " 48248  0.437858  0.402584  0.824245  0.724400  0.454428  0.239489  0.640152   \n",
       " 48252  0.502379  0.447642  0.745363  0.679499  0.442656  0.039419  0.665153   \n",
       " \n",
       "               7         8         9  ...        12        13        14  \\\n",
       " 18     0.540949  0.595594  0.454416  ...  0.307716  0.499794  0.686499   \n",
       " 21     0.498211  0.604367  0.482767  ...  0.346374  0.492241  0.344919   \n",
       " 49     0.492886  0.523809  0.311752  ...  0.445908  0.590731  0.452901   \n",
       " 55     0.649483  0.660774  0.324832  ...  0.392588  0.732014  0.529729   \n",
       " 64     0.681290  0.675319  0.343667  ...  0.412532  0.712970  0.523285   \n",
       " ...         ...       ...       ...  ...       ...       ...       ...   \n",
       " 48239  0.733288  0.590140  0.301693  ...  0.571592  0.556256  0.617118   \n",
       " 48240  0.695220  0.607888  0.323440  ...  0.486586  0.647122  0.426638   \n",
       " 48245  0.566983  0.716507  0.372914  ...  0.552800  0.528479  0.453641   \n",
       " 48248  0.454815  0.649959  0.303085  ...  0.519323  0.541699  0.293103   \n",
       " 48252  0.513339  0.702968  0.311936  ...  0.488717  0.385070  0.422425   \n",
       " \n",
       "              15        16        17        18        19  cluster_ID  labels  \n",
       " 18     0.278749  0.666424  0.483687  0.525669  0.505476           1     M12  \n",
       " 21     0.415745  0.607694  0.348069  0.417216  0.613043           1     E71  \n",
       " 49     0.557286  0.843004  0.472048  0.556383  0.655795           1     C31  \n",
       " 55     0.497870  0.725475  0.429015  0.636399  0.664270           1     M12  \n",
       " 64     0.493018  0.706291  0.417046  0.652102  0.700969           1     M12  \n",
       " ...         ...       ...       ...       ...       ...         ...     ...  \n",
       " 48239  0.416847  0.653905  0.443582  0.544649  0.703695           1     M13  \n",
       " 48240  0.456227  0.751614  0.426683  0.684469  0.576356           1     M13  \n",
       " 48245  0.446029  0.765553  0.499287  0.629369  0.570598           1    GDIP  \n",
       " 48248  0.409883  0.733874  0.431584  0.505062  0.608665           1    E512  \n",
       " 48252  0.348158  0.718916  0.435656  0.619487  0.637272           1     E11  \n",
       " \n",
       " [6265 rows x 22 columns],\n",
       "               0         1         2         3         4         5         6  \\\n",
       " 0      0.622538  0.545301  0.752230  0.619713  0.468593  0.576692  0.548873   \n",
       " 1      0.685125  0.524099  0.719955  0.669576  0.425134  0.459874  0.579058   \n",
       " 25     0.616863  0.602952  0.656970  0.813489  0.353528  0.446601  0.423057   \n",
       " 59     0.585760  0.486472  0.636669  0.659743  0.236465  0.382276  0.550538   \n",
       " 120    0.726874  0.487938  0.732040  0.612896  0.510518  0.443609  0.492116   \n",
       " ...         ...       ...       ...       ...       ...       ...       ...   \n",
       " 48181  0.889883  0.517455  0.706039  0.670609  0.278739  0.311426  0.416162   \n",
       " 48193  0.672441  0.457072  0.686476  0.605027  0.392568  0.230004  0.700699   \n",
       " 48194  0.594634  0.572895  0.589727  0.671302  0.435002  0.363084  0.637657   \n",
       " 48244  0.627858  0.606286  0.667377  0.696054  0.320237  0.397699  0.463322   \n",
       " 48246  0.670242  0.624670  0.650151  0.613210  0.323962  0.490717  0.396884   \n",
       " \n",
       "               7         8         9  ...        12        13        14  \\\n",
       " 0      0.620881  0.475798  0.055148  ...  0.461509  0.519127  0.273841   \n",
       " 1      0.727017  0.528013  0.267745  ...  0.472078  0.627598  0.290054   \n",
       " 25     0.645841  0.584351  0.334889  ...  0.539790  0.502178  0.370629   \n",
       " 59     0.697701  0.518725  0.225142  ...  0.591163  0.689354  0.389111   \n",
       " 120    0.553142  0.390193  0.223432  ...  0.462823  0.432894  0.337026   \n",
       " ...         ...       ...       ...  ...       ...       ...       ...   \n",
       " 48181  0.582313  0.389056  0.328727  ...  0.439147  0.622752  0.246437   \n",
       " 48193  0.505222  0.528845  0.236279  ...  0.485434  0.479829  0.173248   \n",
       " 48194  0.587829  0.419909  0.316932  ...  0.460529  0.574847  0.214437   \n",
       " 48244  0.589820  0.484426  0.254668  ...  0.310749  0.584351  0.409107   \n",
       " 48246  0.657180  0.403545  0.170059  ...  0.303484  0.585609  0.308954   \n",
       " \n",
       "              15        16        17        18        19  cluster_ID  labels  \n",
       " 0      0.269771  0.746221  0.503637  0.479970  0.662316           2    C181  \n",
       " 1      0.466580  0.787930  0.253786  0.502149  0.637086           2    C173  \n",
       " 25     0.285262  0.772390  0.312032  0.683420  0.542419           2     C33  \n",
       " 59     0.309238  0.782342  0.401579  0.513997  0.643294           2     C18  \n",
       " 120    0.353174  0.778297  0.450785  0.540780  0.563499           2     C13  \n",
       " ...         ...       ...       ...       ...       ...         ...     ...  \n",
       " 48181  0.449550  0.712723  0.341111  0.733431  0.340978           2     C12  \n",
       " 48193  0.412421  0.688534  0.413073  0.694566  0.594756           2     C13  \n",
       " 48194  0.515821  0.823872  0.488714  0.790192  0.438215           2     C21  \n",
       " 48244  0.250809  0.835688  0.424322  0.653591  0.513342           2    C171  \n",
       " 48246  0.334991  0.821123  0.424897  0.673453  0.604395           2     C31  \n",
       " \n",
       " [6253 rows x 22 columns],\n",
       "               0         1         2         3         4         5         6  \\\n",
       " 12     0.530324  0.370445  0.720915  0.727739  0.502384  0.491779  0.512216   \n",
       " 19     0.460419  0.434847  0.794412  0.663291  0.741440  0.521486  0.603372   \n",
       " 20     0.574024  0.335885  0.810099  0.563639  0.726408  0.550672  0.572208   \n",
       " 45     0.522483  0.325073  0.697019  0.545456  0.164042  0.298530  0.419715   \n",
       " 57     0.423232  0.291667  0.663764  0.682251  0.248778  0.414845  0.436339   \n",
       " ...         ...       ...       ...       ...       ...       ...       ...   \n",
       " 48179  0.462473  0.439858  0.748443  0.646611  0.324437  0.393909  0.367967   \n",
       " 48208  0.600612  0.423358  0.750951  0.664202  0.206434  0.367110  0.487834   \n",
       " 48222  0.503497  0.446235  0.765445  0.648666  0.228645  0.443968  0.334968   \n",
       " 48228  0.491740  0.504392  0.751418  0.668552  0.214426  0.403859  0.421533   \n",
       " 48255  0.638575  0.366328  0.765155  0.659891  0.389986  0.473013  0.450217   \n",
       " \n",
       "               7         8         9  ...        12        13        14  \\\n",
       " 12     0.501229  0.427255  0.465427  ...  0.354723  0.465410  0.511842   \n",
       " 19     0.190311  0.235887  0.501460  ...  0.366419  0.945248  0.297726   \n",
       " 20     0.354503  0.382551  0.466185  ...  0.420086  0.826536  0.248839   \n",
       " 45     0.629773  0.463425  0.302286  ...  0.531847  0.637935  0.420744   \n",
       " 57     0.630037  0.390175  0.405859  ...  0.567017  0.593029  0.354552   \n",
       " ...         ...       ...       ...  ...       ...       ...       ...   \n",
       " 48179  0.733448  0.505207  0.347082  ...  0.290456  0.709242  0.532874   \n",
       " 48208  0.467080  0.528031  0.364176  ...  0.428201  0.732635  0.399334   \n",
       " 48222  0.626951  0.528610  0.344832  ...  0.351145  0.604508  0.372796   \n",
       " 48228  0.505957  0.561188  0.347959  ...  0.405985  0.633074  0.341079   \n",
       " 48255  0.629111  0.430586  0.237229  ...  0.405036  0.648784  0.371451   \n",
       " \n",
       "              15        16        17        18        19  cluster_ID  labels  \n",
       " 12     0.483391  0.753172  0.541063  0.346387  0.497962           3     M14  \n",
       " 19     0.136577  0.676481  0.817940  0.357580  0.626007           3     C15  \n",
       " 20     0.142761  0.780024  0.652026  0.525243  0.630779           3     C15  \n",
       " 45     0.436872  0.807233  0.374917  0.604472  0.527133           3     C15  \n",
       " 57     0.395257  0.821816  0.332591  0.533747  0.549880           3     C15  \n",
       " ...         ...       ...       ...       ...       ...         ...     ...  \n",
       " 48179  0.373565  0.776906  0.425462  0.529661  0.501282           3     C15  \n",
       " 48208  0.406344  0.729670  0.337355  0.516766  0.588546           3     C15  \n",
       " 48222  0.349382  0.710718  0.344610  0.645786  0.517942           3     C15  \n",
       " 48228  0.446156  0.686861  0.303343  0.529805  0.456361           3     C15  \n",
       " 48255  0.370279  0.764302  0.352685  0.612038  0.606288           3    C151  \n",
       " \n",
       " [6018 rows x 22 columns],\n",
       "               0         1         2         3         4         5         6  \\\n",
       " 3      0.443644  0.521659  0.757898  0.549458  0.224391  0.182809  0.311964   \n",
       " 4      0.409186  0.572101  0.740241  0.530068  0.213700  0.212124  0.312620   \n",
       " 5      0.467565  0.524385  0.704485  0.626329  0.191321  0.286722  0.479602   \n",
       " 13     0.369524  0.405317  0.785525  0.589677  0.297051  0.259080  0.473186   \n",
       " 24     0.396685  0.465590  0.714326  0.499296  0.333432  0.272155  0.427706   \n",
       " ...         ...       ...       ...       ...       ...       ...       ...   \n",
       " 48241  0.412382  0.601434  0.624758  0.496580  0.215856  0.168838  0.567668   \n",
       " 48249  0.481431  0.478684  0.816859  0.594253  0.342898  0.194555  0.427395   \n",
       " 48253  0.461805  0.507951  0.807938  0.726963  0.396872  0.349162  0.488948   \n",
       " 48254  0.484628  0.382649  0.789809  0.634588  0.284122  0.431322  0.411410   \n",
       " 48256  0.446863  0.457169  0.806865  0.654733  0.391600  0.237564  0.338513   \n",
       " \n",
       "               7         8         9  ...        12        13        14  \\\n",
       " 3      0.773963  0.556796  0.278611  ...  0.460008  0.650015  0.460365   \n",
       " 4      0.744341  0.420685  0.306090  ...  0.460553  0.580583  0.396706   \n",
       " 5      0.707901  0.592693  0.228486  ...  0.367715  0.581735  0.562553   \n",
       " 13     0.716607  0.694911  0.177024  ...  0.383679  0.537490  0.466646   \n",
       " 24     0.846342  0.560976  0.294229  ...  0.473231  0.709561  0.470901   \n",
       " ...         ...       ...       ...  ...       ...       ...       ...   \n",
       " 48241  0.603472  0.665673  0.528335  ...  0.346294  0.581712  0.459322   \n",
       " 48249  0.688628  0.785695  0.164253  ...  0.498814  0.415676  0.592777   \n",
       " 48253  0.719884  0.636753  0.152457  ...  0.429557  0.667814  0.506923   \n",
       " 48254  0.768081  0.600158  0.194160  ...  0.397378  0.557770  0.333110   \n",
       " 48256  0.763725  0.566826  0.269333  ...  0.419918  0.559138  0.492278   \n",
       " \n",
       "              15        16        17        18        19  cluster_ID  labels  \n",
       " 3      0.419992  0.722694  0.489154  0.746901  0.500162           4     M11  \n",
       " 4      0.394423  0.767854  0.383323  0.809262  0.548098           4    G152  \n",
       " 5      0.367500  0.758352  0.543592  0.735248  0.423818           4     M11  \n",
       " 13     0.252983  0.668023  0.612992  0.624452  0.558310           4     M11  \n",
       " 24     0.450266  0.725825  0.493529  0.696125  0.668024           4     M11  \n",
       " ...         ...       ...       ...       ...       ...         ...     ...  \n",
       " 48241  0.202343  0.635386  0.422492  0.546813  0.414924           4     C15  \n",
       " 48249  0.405325  0.656691  0.462502  0.548546  0.588779           4     M13  \n",
       " 48253  0.356770  0.623880  0.574491  0.631555  0.536318           4     M11  \n",
       " 48254  0.300557  0.670753  0.411617  0.697991  0.458772           4     M11  \n",
       " 48256  0.352630  0.650700  0.336175  0.685846  0.402290           4     M11  \n",
       " \n",
       " [4576 rows x 22 columns],\n",
       "               0         1         2         3         4         5         6  \\\n",
       " 8      0.527385  0.402567  0.766315  0.706327  0.393040  0.339720  0.636153   \n",
       " 9      0.509594  0.784179  0.579811  0.827575  0.338197  0.359700  0.470636   \n",
       " 10     0.746735  0.567182  0.710020  0.888697  0.308523  0.599976  0.479824   \n",
       " 11     0.647116  0.564338  0.578774  0.708723  0.355835  0.390787  0.581627   \n",
       " 23     0.606139  0.567360  0.688132  0.742855  0.126389  0.323931  0.247881   \n",
       " ...         ...       ...       ...       ...       ...       ...       ...   \n",
       " 48211  0.528694  0.376320  0.772675  0.794226  0.262366  0.346673  0.358260   \n",
       " 48214  0.760290  0.354069  0.574543  0.809003  0.379137  0.468211  0.325024   \n",
       " 48223  0.612059  0.394769  0.716560  0.844769  0.268404  0.385457  0.374123   \n",
       " 48225  0.657468  0.322297  0.761788  0.819418  0.365586  0.549361  0.158069   \n",
       " 48232  0.465295  0.345547  0.844766  0.842701  0.351604  0.552977  0.260319   \n",
       " \n",
       "               7         8         9  ...        12        13        14  \\\n",
       " 8      0.409325  0.652466  0.123355  ...  0.513258  0.483135  0.540719   \n",
       " 9      0.670590  0.538902  0.120249  ...  0.502386  0.331548  0.527434   \n",
       " 10     0.705306  0.578927  0.237672  ...  0.557238  0.442411  0.392244   \n",
       " 11     0.484469  0.772928  0.231713  ...  0.550213  0.672856  0.670896   \n",
       " 23     0.688775  0.616193  0.282287  ...  0.452735  0.646885  0.582329   \n",
       " ...         ...       ...       ...  ...       ...       ...       ...   \n",
       " 48211  0.448002  0.607920  0.345191  ...  0.425358  0.460204  0.416650   \n",
       " 48214  0.635666  0.627305  0.188730  ...  0.499956  0.641425  0.543392   \n",
       " 48223  0.390932  0.587996  0.283719  ...  0.425970  0.577367  0.298033   \n",
       " 48225  0.583934  0.557263  0.260599  ...  0.466140  0.497618  0.479900   \n",
       " 48232  0.511695  0.627663  0.307160  ...  0.447485  0.468947  0.350062   \n",
       " \n",
       "              15        16        17        18        19  cluster_ID  labels  \n",
       " 8      0.342174  0.733299  0.609073  0.624460  0.381463           5    GCAT  \n",
       " 9      0.469705  0.795298  0.567793  0.541803  0.507514           5    GCAT  \n",
       " 10     0.449720  0.786241  0.353597  0.593304  0.461209           5    GCAT  \n",
       " 11     0.365906  0.764447  0.614124  0.580131  0.409703           5    GCAT  \n",
       " 23     0.351042  0.854127  0.437755  0.437432  0.220514           5    GCAT  \n",
       " ...         ...       ...       ...       ...       ...         ...     ...  \n",
       " 48211  0.566483  0.809152  0.405190  0.580689  0.504912           5    GCAT  \n",
       " 48214  0.480767  0.732995  0.538928  0.469754  0.457056           5    GCAT  \n",
       " 48223  0.432718  0.742513  0.481889  0.603698  0.518109           5    GCAT  \n",
       " 48225  0.498146  0.803698  0.512633  0.600613  0.497251           5    GCAT  \n",
       " 48232  0.511531  0.812240  0.560600  0.683805  0.360224           5    GCAT  \n",
       " \n",
       " [5168 rows x 22 columns],\n",
       "               0         1         2         3         4         5         6  \\\n",
       " 357    0.512531  0.318264  0.511863  0.454618  0.336400  0.697740  0.168432   \n",
       " 358    0.484537  0.321180  0.573580  0.494867  0.281750  0.680503  0.262570   \n",
       " 359    0.423203  0.593836  0.719520  0.610651  0.452898  0.669221  0.230228   \n",
       " 360    0.446042  0.447820  0.672692  0.620192  0.488099  0.542541  0.325197   \n",
       " 361    0.640566  0.361996  0.548858  0.606819  0.431783  0.412274  0.206552   \n",
       " ...         ...       ...       ...       ...       ...       ...       ...   \n",
       " 47967  0.651265  0.431180  0.519542  0.704004  0.569674  0.446506  0.480675   \n",
       " 47976  0.282292  0.794905  0.525303  0.765930  0.486574  0.662339  0.294664   \n",
       " 48198  0.633688  0.316709  0.572434  0.722015  0.279099  0.393068  0.281064   \n",
       " 48242  0.457923  0.437487  0.672343  0.724844  0.335885  0.379853  0.515908   \n",
       " 48243  0.375678  0.436712  0.672564  0.696051  0.189872  0.388604  0.227104   \n",
       " \n",
       "               7         8         9  ...        12        13        14  \\\n",
       " 357    0.686558  0.505187  0.204919  ...  0.554516  0.634204  0.620668   \n",
       " 358    0.609035  0.481385  0.207578  ...  0.545348  0.671486  0.582869   \n",
       " 359    0.522488  0.580354  0.474879  ...  0.746498  0.533126  0.673415   \n",
       " 360    0.559137  0.588108  0.321038  ...  0.667926  0.592940  0.640983   \n",
       " 361    0.495323  0.517838  0.383775  ...  0.519549  0.410348  0.536665   \n",
       " ...         ...       ...       ...  ...       ...       ...       ...   \n",
       " 47967  0.644337  0.450269  0.429603  ...  0.440683  0.416796  0.739146   \n",
       " 47976  0.603426  0.358501  0.514752  ...  0.420407  0.250625  0.601482   \n",
       " 48198  0.705556  0.549924  0.261540  ...  0.444106  0.798851  0.720984   \n",
       " 48242  0.732815  0.558809  0.273063  ...  0.473008  0.675075  0.616187   \n",
       " 48243  0.646918  0.609630  0.384742  ...  0.561717  0.596381  0.473365   \n",
       " \n",
       "              15        16        17        18        19  cluster_ID  labels  \n",
       " 357    0.420594  0.800950  0.661548  0.586842  0.190823           6    GCAT  \n",
       " 358    0.415680  0.812757  0.605733  0.576273  0.252527           6    GCAT  \n",
       " 359    0.395967  0.864797  0.423219  0.563683  0.456286           6    GCAT  \n",
       " 360    0.391373  0.865135  0.450747  0.703169  0.222464           6    GCAT  \n",
       " 361    0.354554  0.892570  0.506466  0.721716  0.358913           6    GCAT  \n",
       " ...         ...       ...       ...       ...       ...         ...     ...  \n",
       " 47967  0.386791  0.775099  0.427132  0.627798  0.481313           6     C24  \n",
       " 47976  0.484633  0.697478  0.585326  0.448020  0.232689           6    CCAT  \n",
       " 48198  0.498296  0.735305  0.519378  0.628317  0.324485           6    GCAT  \n",
       " 48242  0.497363  0.870367  0.531918  0.600999  0.219483           6    GCAT  \n",
       " 48243  0.448605  0.938577  0.494188  0.520323  0.170683           6    GCAT  \n",
       " \n",
       " [2414 rows x 22 columns],\n",
       "               0         1         2         3         4         5         6  \\\n",
       " 6      0.438475  0.355000  0.709619  0.611688  0.475906  0.400557  0.482770   \n",
       " 7      0.416567  0.388079  0.674631  0.761005  0.554069  0.547452  0.466133   \n",
       " 14     0.405244  0.376946  0.711636  0.659804  0.533157  0.426163  0.513574   \n",
       " 15     0.459013  0.401460  0.725860  0.626350  0.505058  0.408531  0.470787   \n",
       " 16     0.478991  0.381813  0.747417  0.634061  0.483549  0.433431  0.398004   \n",
       " ...         ...       ...       ...       ...       ...       ...       ...   \n",
       " 48173  0.309153  0.358008  0.696066  0.647999  0.429478  0.403835  0.649933   \n",
       " 48175  0.528929  0.304312  0.682542  0.753744  0.409226  0.323412  0.498817   \n",
       " 48186  0.609126  0.306109  0.720849  0.783681  0.384183  0.604102  0.386963   \n",
       " 48197  0.615019  0.350785  0.649846  0.734855  0.474984  0.342305  0.474056   \n",
       " 48217  0.480869  0.468752  0.807923  0.689290  0.549898  0.489518  0.497677   \n",
       " \n",
       "               7         8         9  ...        12        13        14  \\\n",
       " 6      0.839645  0.483355  0.244208  ...  0.373862  0.429569  0.439964   \n",
       " 7      0.602107  0.474001  0.475275  ...  0.315959  0.520939  0.346812   \n",
       " 14     0.731843  0.652145  0.412445  ...  0.436787  0.542829  0.476051   \n",
       " 15     0.648900  0.600153  0.405777  ...  0.425617  0.490723  0.476939   \n",
       " 16     0.644788  0.594933  0.415590  ...  0.436234  0.468725  0.430280   \n",
       " ...         ...       ...       ...  ...       ...       ...       ...   \n",
       " 48173  0.558141  0.513643  0.453805  ...  0.316318  0.485747  0.278125   \n",
       " 48175  0.664601  0.540610  0.300571  ...  0.401404  0.510253  0.291288   \n",
       " 48186  0.737094  0.568629  0.328527  ...  0.410693  0.433400  0.416008   \n",
       " 48197  0.673986  0.445072  0.413322  ...  0.426455  0.520402  0.609148   \n",
       " 48217  0.524237  0.504990  0.266156  ...  0.437636  0.357915  0.252445   \n",
       " \n",
       "              15        16        17        18        19  cluster_ID  labels  \n",
       " 6      0.424730  0.808686  0.467052  0.682212  0.488480           7     M14  \n",
       " 7      0.267628  0.541086  0.631152  0.611064  0.571895           7     M11  \n",
       " 14     0.407272  0.785674  0.511777  0.413254  0.604878           7     M14  \n",
       " 15     0.440805  0.756461  0.495729  0.351783  0.666357           7     M14  \n",
       " 16     0.379205  0.747169  0.487456  0.380968  0.657284           7     M14  \n",
       " ...         ...       ...       ...       ...       ...         ...     ...  \n",
       " 48173  0.476376  0.829819  0.421587  0.675210  0.389059           7     C31  \n",
       " 48175  0.476374  0.730324  0.300854  0.611060  0.645921           7     M14  \n",
       " 48186  0.357127  0.768219  0.414258  0.616944  0.629763           7     M14  \n",
       " 48197  0.423567  0.813350  0.443828  0.600495  0.497498           7     C24  \n",
       " 48217  0.425064  0.851899  0.358120  0.640081  0.544340           7     C31  \n",
       " \n",
       " [4324 rows x 22 columns],\n",
       "               0         1         2         3         4         5         6  \\\n",
       " 182    0.628773  0.496827  0.682844  0.714123  0.353574  0.370269  0.536642   \n",
       " 210    0.825945  0.496919  0.662441  0.568809  0.551588  0.371147  0.454460   \n",
       " 214    0.764505  0.460583  0.630533  0.648436  0.271343  0.307991  0.433876   \n",
       " 251    0.766892  0.517243  0.611075  0.610738  0.495427  0.435842  0.506949   \n",
       " 255    0.673282  0.564935  0.723841  0.738012  0.342566  0.338604  0.465641   \n",
       " ...         ...       ...       ...       ...       ...       ...       ...   \n",
       " 47934  0.629553  0.533949  0.672623  0.599432  0.330654  0.283587  0.254448   \n",
       " 47935  0.666010  0.702664  0.591488  0.735272  0.247748  0.219223  0.466726   \n",
       " 47962  0.762343  0.329793  0.625176  0.759778  0.464538  0.483929  0.537744   \n",
       " 48144  0.602073  0.512939  0.762788  0.801630  0.382115  0.220755  0.588125   \n",
       " 48233  0.619363  0.318393  0.723947  0.683081  0.242479  0.335351  0.498234   \n",
       " \n",
       "               7         8         9  ...        12        13        14  \\\n",
       " 182    0.725556  0.596612  0.345913  ...  0.374111  0.597481  0.537755   \n",
       " 210    0.597549  0.495372  0.320286  ...  0.292125  0.516334  0.404645   \n",
       " 214    0.551044  0.416580  0.279918  ...  0.336857  0.738762  0.344344   \n",
       " 251    0.608203  0.640515  0.296086  ...  0.362357  0.581819  0.493264   \n",
       " 255    0.527015  0.496012  0.292858  ...  0.359012  0.673885  0.484193   \n",
       " ...         ...       ...       ...  ...       ...       ...       ...   \n",
       " 47934  0.816543  0.358850  0.208331  ...  0.301350  0.811811  0.309982   \n",
       " 47935  0.585120  0.559480  0.352392  ...  0.465373  0.741188  0.496588   \n",
       " 47962  0.626350  0.500036  0.183687  ...  0.515764  0.459476  0.495669   \n",
       " 48144  0.586987  0.518065  0.394733  ...  0.419159  0.470145  0.500817   \n",
       " 48233  0.594032  0.728909  0.229340  ...  0.388498  0.682603  0.627030   \n",
       " \n",
       "              15        16        17        18        19  cluster_ID  labels  \n",
       " 182    0.427538  0.898145  0.444647  0.522964  0.320603           8     C32  \n",
       " 210    0.320151  0.808462  0.234545  0.584933  0.238580           8     C23  \n",
       " 214    0.395647  0.784788  0.529092  0.621258  0.481379           8     C12  \n",
       " 251    0.297909  0.869274  0.316971  0.606603  0.305804           8     C23  \n",
       " 255    0.355371  0.758758  0.469558  0.477397  0.509778           8     C12  \n",
       " ...         ...       ...       ...       ...       ...         ...     ...  \n",
       " 47934  0.381919  0.862307  0.709806  0.540342  0.304221           8     C12  \n",
       " 47935  0.384239  0.810086  0.514009  0.646587  0.326361           8     C12  \n",
       " 47962  0.375121  0.827658  0.563529  0.520965  0.286286           8    GCAT  \n",
       " 48144  0.456682  0.739609  0.401417  0.687847  0.329876           8    GCAT  \n",
       " 48233  0.465314  0.830544  0.579986  0.488029  0.259510           8    GCAT  \n",
       " \n",
       " [3571 rows x 22 columns],\n",
       "               0         1         2         3         4         5         6  \\\n",
       " 2      0.502486  0.505510  0.686061  0.556233  0.161481  0.281391  0.133655   \n",
       " 22     0.599474  0.584084  0.620575  0.597707  0.269176  0.347541  0.407602   \n",
       " 27     0.504081  0.324720  0.691331  0.664615  0.330240  0.340879  0.400248   \n",
       " 35     0.548639  0.539367  0.685018  0.618115  0.400919  0.278274  0.288631   \n",
       " 38     0.546855  0.464392  0.661404  0.598954  0.198393  0.264069  0.361319   \n",
       " ...         ...       ...       ...       ...       ...       ...       ...   \n",
       " 48192  0.592204  0.407560  0.623760  0.676718  0.331540  0.335065  0.425457   \n",
       " 48200  0.552016  0.362599  0.706755  0.772783  0.288608  0.353972  0.198661   \n",
       " 48210  0.491465  0.286472  0.790171  0.616034  0.412305  0.489757  0.431475   \n",
       " 48247  0.594646  0.521073  0.606308  0.664774  0.119255  0.405809  0.408329   \n",
       " 48251  0.443658  0.471818  0.781968  0.643702  0.471611  0.473781  0.432841   \n",
       " \n",
       "               7         8         9  ...        12        13        14  \\\n",
       " 2      0.689424  0.568082  0.252426  ...  0.489413  0.623307  0.288272   \n",
       " 22     0.876612  0.493712  0.310902  ...  0.570433  0.609428  0.401843   \n",
       " 27     0.620277  0.635326  0.313258  ...  0.491305  0.701992  0.315127   \n",
       " 35     0.743620  0.432869  0.355497  ...  0.419412  0.487470  0.365799   \n",
       " 38     0.684586  0.700392  0.362770  ...  0.552066  0.632432  0.455847   \n",
       " ...         ...       ...       ...  ...       ...       ...       ...   \n",
       " 48192  0.711226  0.607435  0.288499  ...  0.458787  0.563110  0.355947   \n",
       " 48200  0.565909  0.508539  0.291570  ...  0.479028  0.643749  0.433971   \n",
       " 48210  0.409470  0.539901  0.301845  ...  0.569065  0.513642  0.163387   \n",
       " 48247  0.744865  0.506037  0.290151  ...  0.442336  0.644601  0.313035   \n",
       " 48251  0.653062  0.542020  0.238812  ...  0.497977  0.403686  0.188723   \n",
       " \n",
       "              15        16        17        18        19  cluster_ID  labels  \n",
       " 2      0.325249  0.763247  0.418129  0.672976  0.523832           9     C18  \n",
       " 22     0.399324  0.882129  0.276725  0.693205  0.439133           9     C11  \n",
       " 27     0.348215  0.790524  0.388398  0.653918  0.621715           9     C17  \n",
       " 35     0.472032  0.898224  0.353793  0.688969  0.549593           9     C31  \n",
       " 38     0.336923  0.823647  0.321787  0.621092  0.481984           9     E12  \n",
       " ...         ...       ...       ...       ...       ...         ...     ...  \n",
       " 48192  0.370067  0.701324  0.360963  0.649327  0.521961           9     M11  \n",
       " 48200  0.304331  0.771311  0.433987  0.656233  0.638383           9     C14  \n",
       " 48210  0.367529  0.773152  0.329859  0.714723  0.688988           9     C15  \n",
       " 48247  0.329217  0.811418  0.421248  0.594379  0.578987           9     C33  \n",
       " 48251  0.194835  0.786779  0.273833  0.581640  0.491508           9    C152  \n",
       " \n",
       " [5253 rows x 22 columns]]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clusterDataframeList # Cluster data is maintained in the respective index of the List"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### APPLYING CLASSIFIERS FOR EACH CLUSTERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Networks\n",
      "========================================\n",
      "Accuracy :  0.6172140430351076\n",
      "Classification Report :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         C11       0.00      0.00      0.00         2\n",
      "         C12       0.33      0.33      0.33         3\n",
      "         C13       0.57      0.54      0.56        79\n",
      "         C14       0.00      0.00      0.00         0\n",
      "         C15       0.17      0.33      0.22         3\n",
      "        C151       0.00      0.00      0.00         0\n",
      "         C16       0.00      0.00      0.00         1\n",
      "         C17       0.00      0.00      0.00         2\n",
      "         C18       0.25      0.21      0.23        14\n",
      "        C183       0.00      0.00      0.00         0\n",
      "         C21       0.00      0.00      0.00         7\n",
      "         C23       0.00      0.00      0.00         0\n",
      "         C24       0.18      0.33      0.24         6\n",
      "         C31       0.29      0.46      0.35        13\n",
      "         C34       0.00      0.00      0.00         1\n",
      "         C41       0.25      1.00      0.40         1\n",
      "         C42       0.50      0.40      0.44        45\n",
      "        CCAT       0.00      0.00      0.00         0\n",
      "         E11       0.50      0.58      0.54        55\n",
      "         E12       0.65      0.64      0.64       121\n",
      "         E13       0.62      0.43      0.51        60\n",
      "        E131       0.00      0.00      0.00         0\n",
      "         E14       0.25      0.33      0.29         6\n",
      "         E21       0.57      0.57      0.57        54\n",
      "         E31       0.31      0.36      0.33        11\n",
      "         E41       0.27      0.41      0.32        27\n",
      "         E51       0.46      0.42      0.44        26\n",
      "        E512       0.00      0.00      0.00         1\n",
      "         E61       0.00      0.00      0.00         0\n",
      "         E71       0.96      0.87      0.91        30\n",
      "         G15       0.65      0.65      0.65        20\n",
      "        G151       0.00      0.00      0.00         0\n",
      "        G155       0.00      0.00      0.00         0\n",
      "        GCAT       0.91      0.81      0.86       275\n",
      "        GPOL       0.00      0.00      0.00         0\n",
      "        GVIO       0.00      0.00      0.00         1\n",
      "         M11       1.00      0.91      0.95        11\n",
      "         M12       0.60      0.60      0.60         5\n",
      "         M13       0.00      0.00      0.00         2\n",
      "         M14       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.62       883\n",
      "   macro avg       0.26      0.28      0.26       883\n",
      "weighted avg       0.66      0.62      0.63       883\n",
      "\n",
      "SVC\n",
      "========================================\n",
      "Accuracy :  0.16520351157222665\n",
      "Classification Report :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         C11       0.00      0.00      0.00         0\n",
      "         C12       0.00      0.00      0.00         0\n",
      "         C13       0.00      0.00      0.00         0\n",
      "         C14       0.00      0.00      0.00         0\n",
      "         C15       0.00      0.00      0.00         0\n",
      "         C16       0.00      0.00      0.00         0\n",
      "         C17       0.00      0.00      0.00         0\n",
      "         C18       0.00      0.00      0.00         0\n",
      "        C183       0.00      0.00      0.00         0\n",
      "         C21       0.00      0.00      0.00         0\n",
      "         C22       0.00      0.00      0.00         0\n",
      "         C23       0.00      0.00      0.00         0\n",
      "         C24       0.00      0.00      0.00         0\n",
      "         C31       0.00      0.00      0.00         0\n",
      "         C32       0.00      0.00      0.00         0\n",
      "         C33       0.00      0.00      0.00         0\n",
      "         C34       0.00      0.00      0.00         0\n",
      "         C41       0.00      0.00      0.00         0\n",
      "         E11       0.00      0.00      0.00         0\n",
      "         E12       0.00      0.00      0.00         0\n",
      "         E13       0.00      0.00      0.00         0\n",
      "         E14       0.00      0.00      0.00         0\n",
      "         E21       0.00      0.00      0.00         0\n",
      "         E31       0.00      0.00      0.00         0\n",
      "         E41       0.00      0.00      0.00         0\n",
      "         E51       0.00      0.00      0.00         0\n",
      "        E512       0.00      0.00      0.00         0\n",
      "         E61       0.00      0.00      0.00         0\n",
      "         E71       0.00      0.00      0.00         0\n",
      "         G15       0.00      0.00      0.00         0\n",
      "        G152       0.00      0.00      0.00         0\n",
      "        GCAT       0.00      0.00      0.00         0\n",
      "         M11       0.00      0.00      0.00         0\n",
      "         M12       0.00      0.00      0.00         0\n",
      "         M13       1.00      0.17      0.28      1253\n",
      "         M14       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.17      1253\n",
      "   macro avg       0.03      0.00      0.01      1253\n",
      "weighted avg       1.00      0.17      0.28      1253\n",
      "\n",
      "Decision Trees\n",
      "========================================\n",
      "Accuracy :  0.2837729816147082\n",
      "Classification Report :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         C11       0.00      0.00      0.00         0\n",
      "         C12       0.00      0.00      0.00         0\n",
      "         C13       0.00      0.00      0.00         0\n",
      "         C14       0.00      0.00      0.00         0\n",
      "         C15       0.73      0.41      0.52       561\n",
      "        C151       0.00      0.00      0.00         0\n",
      "        C152       0.00      0.00      0.00         0\n",
      "         C16       0.00      0.00      0.00         0\n",
      "         C17       0.00      0.00      0.00         0\n",
      "         C18       0.52      0.20      0.29       415\n",
      "        C181       0.00      0.00      0.00         0\n",
      "         C21       0.00      0.00      0.00         0\n",
      "         C22       0.00      0.00      0.00         0\n",
      "         C23       0.00      0.00      0.00         0\n",
      "         C24       0.00      0.00      0.00         0\n",
      "         C31       0.00      0.00      0.00         0\n",
      "         C32       0.00      0.00      0.00         0\n",
      "         C33       0.00      0.00      0.00         0\n",
      "         C34       0.00      0.00      0.00         0\n",
      "         C41       0.63      0.15      0.24       275\n",
      "         C42       0.00      0.00      0.00         0\n",
      "        CCAT       0.00      0.00      0.00         0\n",
      "         E11       0.00      0.00      0.00         0\n",
      "         E12       0.00      0.00      0.00         0\n",
      "         E13       0.00      0.00      0.00         0\n",
      "         E14       0.00      0.00      0.00         0\n",
      "         E21       0.00      0.00      0.00         0\n",
      "         E51       0.00      0.00      0.00         0\n",
      "        GCAT       0.00      0.00      0.00         0\n",
      "        GDEF       0.00      0.00      0.00         0\n",
      "        GJOB       0.00      0.00      0.00         0\n",
      "         M11       0.00      0.00      0.00         0\n",
      "         M12       0.00      0.00      0.00         0\n",
      "         M13       0.00      0.00      0.00         0\n",
      "         M14       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.28      1251\n",
      "   macro avg       0.05      0.02      0.03      1251\n",
      "weighted avg       0.64      0.28      0.38      1251\n",
      "\n",
      "Random Forest\n",
      "========================================\n",
      "Accuracy :  0.5780730897009967\n",
      "Classification Report :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         C11       0.00      0.00      0.00         0\n",
      "         C12       0.00      0.00      0.00         0\n",
      "         C13       0.00      0.00      0.00         0\n",
      "         C14       0.00      0.00      0.00         0\n",
      "         C15       1.00      0.58      0.73      1204\n",
      "        C151       0.00      0.00      0.00         0\n",
      "        C152       0.00      0.00      0.00         0\n",
      "         C16       0.00      0.00      0.00         0\n",
      "         C17       0.00      0.00      0.00         0\n",
      "         C18       0.00      0.00      0.00         0\n",
      "        C181       0.00      0.00      0.00         0\n",
      "         C21       0.00      0.00      0.00         0\n",
      "         C22       0.00      0.00      0.00         0\n",
      "         C23       0.00      0.00      0.00         0\n",
      "         C24       0.00      0.00      0.00         0\n",
      "         C31       0.00      0.00      0.00         0\n",
      "         C32       0.00      0.00      0.00         0\n",
      "         C33       0.00      0.00      0.00         0\n",
      "         C34       0.00      0.00      0.00         0\n",
      "         C41       0.00      0.00      0.00         0\n",
      "         C42       0.00      0.00      0.00         0\n",
      "         E12       0.00      0.00      0.00         0\n",
      "         E13       0.00      0.00      0.00         0\n",
      "         E14       0.00      0.00      0.00         0\n",
      "         E21       0.00      0.00      0.00         0\n",
      "         E51       0.00      0.00      0.00         0\n",
      "         E71       0.00      0.00      0.00         0\n",
      "         G15       0.00      0.00      0.00         0\n",
      "        GCAT       0.00      0.00      0.00         0\n",
      "         M11       0.00      0.00      0.00         0\n",
      "         M12       0.00      0.00      0.00         0\n",
      "         M13       0.00      0.00      0.00         0\n",
      "         M14       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.58      1204\n",
      "   macro avg       0.03      0.02      0.02      1204\n",
      "weighted avg       1.00      0.58      0.73      1204\n",
      "\n",
      "KNearestNeighbors\n",
      "=========================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy :  0.732532751091703\n",
      "Classification Report :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         C11       0.00      0.00      0.00         3\n",
      "         C13       0.00      0.00      0.00         2\n",
      "         C14       0.00      0.00      0.00         3\n",
      "         C15       0.61      0.73      0.67        71\n",
      "         C17       0.00      0.00      0.00         1\n",
      "         C18       0.00      0.00      0.00         0\n",
      "         C21       0.00      0.00      0.00         1\n",
      "         C24       0.00      0.00      0.00         1\n",
      "         C31       0.35      0.60      0.44        10\n",
      "         C41       0.00      0.00      0.00         0\n",
      "         E11       0.33      0.22      0.27         9\n",
      "         E12       0.33      0.43      0.38        37\n",
      "         E13       0.00      0.00      0.00         0\n",
      "        E131       0.00      0.00      0.00         0\n",
      "         E14       0.00      0.00      0.00         2\n",
      "         E21       0.25      1.00      0.40         1\n",
      "         E51       0.57      0.80      0.67         5\n",
      "         E71       0.00      0.00      0.00         1\n",
      "         G15       0.00      0.00      0.00         0\n",
      "        G151       0.00      0.00      0.00         0\n",
      "        G154       0.00      0.00      0.00         0\n",
      "        GCAT       0.00      0.00      0.00         0\n",
      "        GENT       0.00      0.00      0.00         0\n",
      "        GPOL       0.00      0.00      0.00         0\n",
      "         M11       0.93      0.79      0.86       415\n",
      "         M12       0.74      0.71      0.72       137\n",
      "         M13       0.81      0.75      0.78       205\n",
      "         M14       0.50      0.92      0.65        12\n",
      "\n",
      "    accuracy                           0.73       916\n",
      "   macro avg       0.19      0.25      0.21       916\n",
      "weighted avg       0.79      0.73      0.76       916\n",
      "\n",
      "Guassian Naive Bayes\n",
      "=========================================\n",
      "Accuracy :  0.8123791102514507\n",
      "Classification Report :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         C11       0.25      0.25      0.25         4\n",
      "         C12       0.57      0.24      0.33        17\n",
      "         C13       0.25      0.45      0.32        11\n",
      "         C15       0.00      0.00      0.00         0\n",
      "         C21       0.00      0.00      0.00         1\n",
      "         C22       0.00      0.00      0.00         1\n",
      "         C24       0.15      0.12      0.14        24\n",
      "         C31       0.33      0.50      0.40         2\n",
      "         C33       0.00      0.00      0.00         0\n",
      "         C41       0.00      0.00      0.00         0\n",
      "         C42       0.25      0.25      0.25         4\n",
      "        CCAT       0.00      0.00      0.00        18\n",
      "         E11       1.00      0.33      0.50         3\n",
      "         E12       0.67      1.00      0.80         2\n",
      "         E21       0.33      0.33      0.33         3\n",
      "         E41       0.00      0.00      0.00         2\n",
      "         E51       0.17      0.35      0.23        20\n",
      "        ECAT       1.00      0.50      0.67         2\n",
      "         G15       0.63      0.50      0.56        48\n",
      "        G155       0.00      0.00      0.00         1\n",
      "        GCAT       0.91      0.91      0.91       867\n",
      "        GDIP       0.00      0.00      0.00         1\n",
      "        GDIS       0.00      0.00      0.00         1\n",
      "        GPOL       0.00      0.00      0.00         0\n",
      "        GVIO       0.00      0.00      0.00         0\n",
      "         M14       1.00      0.50      0.67         2\n",
      "\n",
      "    accuracy                           0.81      1034\n",
      "   macro avg       0.29      0.24      0.24      1034\n",
      "weighted avg       0.83      0.81      0.82      1034\n",
      "\n",
      "Guassian Multinomial Naive Bayes\n",
      "=========================================\n",
      "Accuracy :  0.937888198757764\n",
      "Classification Report :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         C12       0.00      0.00      0.00         0\n",
      "         C15       0.00      0.00      0.00         0\n",
      "         C21       0.00      0.00      0.00         0\n",
      "         C22       0.00      0.00      0.00         0\n",
      "         C24       0.00      0.00      0.00         0\n",
      "        CCAT       0.00      0.00      0.00         0\n",
      "        GCAT       1.00      0.94      0.97       483\n",
      "        GDIP       0.00      0.00      0.00         0\n",
      "         M11       0.00      0.00      0.00         0\n",
      "         M12       0.00      0.00      0.00         0\n",
      "         M13       0.00      0.00      0.00         0\n",
      "         M14       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.94       483\n",
      "   macro avg       0.08      0.08      0.08       483\n",
      "weighted avg       1.00      0.94      0.97       483\n",
      "\n",
      "Stochastic Gradient Descent\n",
      "=========================================\n",
      "Accuracy :  0.7179190751445087\n",
      "Classification Report :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         C11       0.00      0.00      0.00         0\n",
      "         C13       0.00      0.00      0.00         0\n",
      "         C14       0.00      0.00      0.00         0\n",
      "         C15       0.62      0.67      0.64        12\n",
      "         C18       0.00      0.00      0.00         0\n",
      "         C21       0.31      0.68      0.42        38\n",
      "         C22       0.00      0.00      0.00         0\n",
      "         C24       0.16      0.50      0.24        20\n",
      "         C31       0.44      0.32      0.37        85\n",
      "         C33       0.00      0.00      0.00         0\n",
      "         C42       0.00      0.00      0.00         0\n",
      "        CCAT       0.00      0.00      0.00         0\n",
      "         E13       0.00      0.00      0.00         0\n",
      "         E14       0.00      0.00      0.00         0\n",
      "         E51       0.00      0.00      0.00         1\n",
      "        E512       0.00      0.00      0.00         0\n",
      "        GCAT       0.00      0.00      0.00         0\n",
      "         M11       0.80      0.36      0.50        11\n",
      "         M13       0.00      0.00      0.00         0\n",
      "         M14       0.97      0.78      0.87       698\n",
      "        M141       0.00      0.00      0.00         0\n",
      "        MCAT       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.72       865\n",
      "   macro avg       0.15      0.15      0.14       865\n",
      "weighted avg       0.87      0.72      0.78       865\n",
      "\n",
      "ADA-Boost\n",
      "=========================================\n",
      "Accuracy :  0.4671328671328671\n",
      "Classification Report :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         C11       0.00      0.00      0.00         0\n",
      "         C12       0.00      0.00      0.00         0\n",
      "         C13       0.00      0.00      0.00         0\n",
      "         C15       0.00      0.00      0.00         0\n",
      "         C17       0.00      0.00      0.00         0\n",
      "         C18       0.00      0.00      0.00         0\n",
      "         C21       0.00      0.00      0.00         0\n",
      "         C22       0.00      0.00      0.00         0\n",
      "         C23       0.00      0.00      0.00         0\n",
      "         C24       0.00      0.00      0.00         0\n",
      "         C31       0.00      0.00      0.00         0\n",
      "         C32       0.00      0.00      0.00         0\n",
      "         C33       0.00      0.00      0.00         0\n",
      "         C41       0.00      0.00      0.00         0\n",
      "         C42       0.00      0.00      0.00         0\n",
      "        CCAT       0.00      0.00      0.00         0\n",
      "         E11       0.00      0.00      0.00         0\n",
      "         E12       0.00      0.00      0.00         0\n",
      "         E13       0.00      0.00      0.00         0\n",
      "         E21       0.80      0.36      0.50       306\n",
      "         E41       0.00      0.00      0.00         0\n",
      "         E51       0.00      0.00      0.00         0\n",
      "         G15       0.00      0.00      0.00         0\n",
      "        GCAT       0.82      0.55      0.66       408\n",
      "        GENV       0.00      0.00      0.00         1\n",
      "        GPOL       0.00      0.00      0.00         0\n",
      "         M11       0.00      0.00      0.00         0\n",
      "         M12       0.00      0.00      0.00         0\n",
      "         M13       0.00      0.00      0.00         0\n",
      "         M14       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.47       715\n",
      "   macro avg       0.05      0.03      0.04       715\n",
      "weighted avg       0.81      0.47      0.59       715\n",
      "\n",
      "Bagging\n",
      "=========================================\n",
      "Accuracy :  0.5185537583254044\n",
      "Classification Report :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         C11       0.41      0.28      0.33       166\n",
      "         C12       0.88      0.64      0.74        11\n",
      "         C13       0.16      0.58      0.25        12\n",
      "         C14       0.00      0.00      0.00         0\n",
      "         C15       0.88      0.62      0.73       534\n",
      "        C151       0.00      0.00      0.00         0\n",
      "        C152       0.00      0.00      0.00         0\n",
      "         C16       0.00      0.00      0.00         1\n",
      "         C17       0.07      0.75      0.12         4\n",
      "         C18       0.58      0.47      0.52       199\n",
      "        C181       0.00      0.00      0.00         1\n",
      "        C183       0.00      0.00      0.00         0\n",
      "         C21       0.11      0.33      0.16        12\n",
      "         C22       0.17      1.00      0.29         1\n",
      "         C23       0.00      0.00      0.00         0\n",
      "         C24       0.26      0.56      0.35        16\n",
      "         C31       0.27      0.57      0.37        23\n",
      "         C32       0.00      0.00      0.00         0\n",
      "         C33       0.07      0.15      0.10        13\n",
      "         C41       0.00      0.00      0.00         0\n",
      "         C42       0.12      0.14      0.13         7\n",
      "        CCAT       0.00      0.00      0.00         0\n",
      "         E11       0.00      0.00      0.00         0\n",
      "         E12       0.00      0.00      0.00         0\n",
      "         E14       0.00      0.00      0.00         0\n",
      "         E21       0.00      0.00      0.00         1\n",
      "         E31       0.00      0.00      0.00         0\n",
      "         E51       0.00      0.00      0.00         0\n",
      "        E512       0.00      0.00      0.00         0\n",
      "         G15       0.00      0.00      0.00         0\n",
      "        G152       0.00      0.00      0.00         0\n",
      "        GCAT       0.37      0.50      0.42        14\n",
      "       GCRIM       0.00      0.00      0.00         0\n",
      "         M11       0.61      0.59      0.60        34\n",
      "         M12       0.00      0.00      0.00         0\n",
      "         M14       0.17      0.50      0.25         2\n",
      "\n",
      "    accuracy                           0.52      1051\n",
      "   macro avg       0.14      0.21      0.15      1051\n",
      "weighted avg       0.67      0.52      0.57      1051\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def applyClassifier(receivedDFList):\n",
    "    for df in receivedDFList:\n",
    "        clusterNumber = df['cluster_ID'].unique()\n",
    "        clusterNumber = clusterNumber[0]\n",
    "        \n",
    "        Xtr, Xte, Ytr, Yte, target = trainTestSplit(df)\n",
    "        #\n",
    "        if clusterNumber == 0:\n",
    "            # Artificial Neural Network(ANN) uses the processing of the brain as a basis \n",
    "            # to develop algorithms that can be used to model complex patterns and prediction problems.\n",
    "            print(\"Neural Networks\")\n",
    "            print(\"========================================\")\n",
    "            neuralNetwork_model = MLPClassifier(solver='lbfgs', alpha=1e-5,\n",
    "                                                hidden_layer_sizes=(60,), random_state=1, max_iter=500)\n",
    "            trainedClassifier = neuralNetwork_model.fit(Xtr, Ytr)\n",
    "#SVM constructs a hyperplane in multidimensional space to separate different classes. \n",
    "#SVM generates optimal hyperplane in an iterative manner, which is used to minimize an error.\n",
    "        elif clusterNumber == 1:\n",
    "            print(\"SVC\")\n",
    "            print(\"========================================\")\n",
    "            SVC_model = SVC(kernel='sigmoid', gamma=0.1, C=0.1)\n",
    "            trainedClassifier = SVC_model.fit(Xtr, Ytr)\n",
    "\n",
    "        elif clusterNumber == 2:\n",
    "            #A decision tree is a flowchart-like structure in which each internal node represents a “test” on an attribute\n",
    "            # each branch represents the outcome of the test, and each leaf node represents a class label.\n",
    "            #The paths from root to leaf represent classification rules.\n",
    "            print(\"Decision Trees\")\n",
    "            print(\"========================================\")\n",
    "            decionTree_model = DecisionTreeClassifier(criterion='entropy', max_depth=5, min_samples_split=0.2,\n",
    "                                                      min_samples_leaf=0.2)\n",
    "            trainedClassifier = decionTree_model.fit(Xtr, Ytr)\n",
    "            \n",
    "        # Random forest is an ensemble method in which a classifier is constructed by combining several different Independent base classifiers.\n",
    "\n",
    "        elif clusterNumber == 3:\n",
    "            print(\"Random Forest\")\n",
    "            print(\"========================================\")\n",
    "            randomForest_model = RandomForestClassifier(n_estimators=10, max_depth=3, min_samples_split=0.4,\n",
    "                                                        min_samples_leaf=0.2)\n",
    "            trainedClassifier = randomForest_model.fit(Xtr, Ytr)\n",
    "        # KNN is a non-parametric and lazy learning algorithm. Non-parametric means there is no assumption for underlying data distribution.\n",
    "        elif clusterNumber == 4:\n",
    "            print(\"KNearestNeighbors\")\n",
    "            print(\"=========================================\")\n",
    "            KNN_model = KNeighborsClassifier(n_neighbors=5)\n",
    "            trainedClassifier = KNN_model.fit(Xtr, Ytr)\n",
    "        # Naive Bayes is a classification algorithm for binary (two-class) and multi-class classification problems.\n",
    "        elif clusterNumber == 5:\n",
    "            print(\"Guassian Naive Bayes\")\n",
    "            print(\"=========================================\")\n",
    "            GNB_model = GaussianNB()\n",
    "            trainedClassifier = GNB_model.fit(Xtr, Ytr)\n",
    "        # Multinomial Naive Bayes calculates likelihood to be count of an word/token \n",
    "\n",
    "        elif clusterNumber == 6:\n",
    "            print(\"Guassian Multinomial Naive Bayes\")\n",
    "            print(\"=========================================\")\n",
    "            GMNB_model = MultinomialNB()\n",
    "            trainedClassifier = GMNB_model.fit(Xtr, Ytr)\n",
    "\n",
    "        # We use only a single training example for calculation of gradient and update parameters.\n",
    "        elif clusterNumber == 7:\n",
    "            print(\"Stochastic Gradient Descent\")\n",
    "            print(\"=========================================\")\n",
    "            SGD_model = SGDClassifier(loss='modified_huber',shuffle=True,random_state=101)\n",
    "            trainedClassifier = SGD_model.fit(Xtr, Ytr)\n",
    "\n",
    "        # AdaBoost is a popular boosting technique which helps you combine multiple “weak classifiers” into a single “strong classifier”.\n",
    "        elif clusterNumber == 8:\n",
    "            print(\"ADA-Boost\")\n",
    "            print(\"=========================================\")\n",
    "            ADAB_model = AdaBoostClassifier(n_estimators=50,learning_rate=1)\n",
    "            trainedClassifier = ADAB_model.fit(Xtr, Ytr)\n",
    "        #It’s a sub-class of ensemble machine learning algorithms wherein we use multiple weak models and aggregate the predictions we get from each of them to get the final prediction. \n",
    "        elif clusterNumber == 9:\n",
    "            print(\"Bagging\")\n",
    "            print(\"=========================================\")\n",
    "            Bagging_model = BaggingClassifier(n_estimators=50)\n",
    "            trainedClassifier = Bagging_model.fit(Xtr, Ytr)\n",
    "\n",
    "\n",
    "        else:\n",
    "            print(\"end\")\n",
    "            break\n",
    "            \n",
    "        calculateMetrics(trainedClassifier, Xte, Yte)\n",
    "    return target\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "def trainTestSplit(dataFrame):\n",
    "    target = dataFrame['labels']\n",
    "    splitDF = dataFrame.iloc[:,:-1]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(splitDF, target, test_size=0.2, random_state=101)\n",
    "    return X_train, X_test, y_train, y_test, target\n",
    "\n",
    "def calculateMetrics(trainedReceivedClassifier, XtestData, YtestData):\n",
    "    predictor = trainedReceivedClassifier.predict(XtestData)\n",
    "    confusionMatrix = confusion_matrix(predictor, YtestData)\n",
    "    accuracy = accuracy_score(predictor, YtestData)\n",
    "    ClassificationReport = classification_report(predictor, YtestData)\n",
    "    print(\"Accuracy : \", accuracy)\n",
    "    print(\"Classification Report :\")\n",
    "    print(ClassificationReport)\n",
    "    \n",
    "    \n",
    "target = applyClassifier(frameListforEnhance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MEASURING QUALITY OF CLUSTERING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster Quality Scores\n",
      "with ground truth labels\n",
      "==========================\n",
      "Adjusted Rand index\n",
      "0.2070943083585578\n",
      "Mutual Information based scores\n",
      "0.2906931087278954\n",
      "Homogeneity score\n",
      "0.2922848919683959\n",
      "completeness score\n",
      "0.3652891571282194\n",
      "V Measure Score\n",
      "0.3247345359055094\n",
      "Fowlkes-Mallows scores\n",
      "0.2874215022954677\n",
      "\n",
      "\n",
      "\n",
      "without ground truth labels\n",
      "============================\n",
      "silhouette\n",
      "0.05735247516532827\n"
     ]
    }
   ],
   "source": [
    "def clusterQuality(receivedclusters,receivedfeatureData,receivedfeatureDataFrame):\n",
    "    receivedbipTopics = receivedfeatureDataFrame.labels\n",
    "    # The knowledge of ground truth classes is known and hence the measure taking those classes into consideration is used\n",
    "    # To compare both evaluation based on ground truth tables and independent of them, silhouette score is used\n",
    "    # which resulted in biased scoring.\n",
    "    \n",
    "    \n",
    "    print(\"Cluster Quality Scores\")\n",
    "    print(\"with ground truth labels\")\n",
    "    print(\"==========================\")\n",
    "    \n",
    "    #The Rand Index computes a similarity measure between two clusterings by considering all pairs of samples \n",
    "    #and counting pairs that are assigned in the same or different clusters in the predicted and true clusterings.\n",
    "    print(\"Adjusted Rand index\")\n",
    "    print(metrics.adjusted_rand_score(receivedbipTopics, receivedclusters))\n",
    "    \n",
    "    \n",
    "    #The Mutual Information is a measure of the similarity between two labels of the same data.\n",
    "    #This Mutualinformation score is useful to check whether the clustering algorithm meets an important requirement:\n",
    "    #a cluster should contain only samples belonging to a single class.\n",
    "    print(\"Mutual Information based scores\")\n",
    "    print(metrics.adjusted_mutual_info_score(receivedbipTopics, receivedclusters))\n",
    "    \n",
    "    \n",
    "    #A perfectly homogeneous clustering is one where each cluster has data-points belonging to the same class label. \n",
    "    #Homogeneity describes the closeness of the clustering algorithm to this perfection.\n",
    "    print(\"Homogeneity score\")\n",
    "    print(metrics.homogeneity_score(receivedbipTopics, receivedclusters))\n",
    "    \n",
    "    \n",
    "    #Completness score purpose is to provide a piece of information about the assignment of samples belonging to the same class.\n",
    "    #More precisely, a good clustering algorithm should assign all samples with the same true label to the same cluster.\n",
    "    print(\"completeness score\")\n",
    "    print(metrics.completeness_score(receivedbipTopics, receivedclusters))\n",
    "    \n",
    "    \n",
    "    #The V-Measure is defined as the harmonic mean of homogeneity and completeness of the clustering\n",
    "    # No assumption is made on the cluster structure: can be used to compare clustering algorithms such as k-means which assumes isotropic \n",
    "    # blob shapes with results of spectral clustering algorithms which can find cluster with “folded” shapes.\n",
    "    print(\"V Measure Score\")\n",
    "    print(metrics.v_measure_score(receivedbipTopics, receivedclusters))\n",
    "    \n",
    "    \n",
    "    #The Fowlkes-Mallows Score is an evaluation metric to evaluate the similarity among clusterings obtained after applying different clustering algorithms. \n",
    "    print(\"Fowlkes-Mallows scores\")\n",
    "    print(metrics.fowlkes_mallows_score(receivedbipTopics, receivedclusters))\n",
    "    print(\"\\n\\n\")\n",
    "    print(\"without ground truth labels\")\n",
    "    print(\"============================\")\n",
    "    print(\"silhouette\")\n",
    "    print(metrics.silhouette_score(receivedfeatureData, receivedclusters, metric='euclidean'))\n",
    "    \n",
    "clusterQuality(receivedClusters, receivedFeatureData, receivedClusterDataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FEATURE EXTRACTION USING AUTO-ENCODERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enhancing Features using AutoEncoder..........\n",
      "Enhancing Features using AutoEncoder..........\n",
      "Enhancing Features using AutoEncoder..........\n",
      "Enhancing Features using AutoEncoder..........\n",
      "Enhancing Features using AutoEncoder..........\n",
      "Enhancing Features using AutoEncoder..........\n",
      "Enhancing Features using AutoEncoder..........\n",
      "Enhancing Features using AutoEncoder..........\n",
      "Enhancing Features using AutoEncoder..........\n",
      "Enhancing Features using AutoEncoder..........\n",
      "[             0         1         2         3         4         5         6  \\\n",
      "0     0.612590  0.308493  0.717187  0.682659  0.362452  0.153676  0.408199   \n",
      "1     0.528015  0.401692  0.726005  0.640563  0.361598  0.247508  0.436721   \n",
      "2     0.581098  0.482747  0.718352  0.694340  0.379208  0.253293  0.387036   \n",
      "3     0.493961  0.421248  0.764261  0.511699  0.293629  0.158896  0.528732   \n",
      "4     0.590432  0.362342  0.721932  0.588542  0.329968  0.159757  0.358458   \n",
      "...        ...       ...       ...       ...       ...       ...       ...   \n",
      "4410  0.513271  0.323983  0.619513  0.596799  0.284693  0.205667  0.516785   \n",
      "4411  0.492969  0.308827  0.632944  0.595691  0.281446  0.203478  0.519000   \n",
      "4412  0.427721  0.348545  0.759183  0.655163  0.341489  0.293501  0.561487   \n",
      "4413  0.627134  0.363561  0.667065  0.711199  0.386204  0.191085  0.410475   \n",
      "4414  0.538576  0.450741  0.681119  0.671631  0.295191  0.230916  0.516465   \n",
      "\n",
      "             7         8         9  ...        12        13        14  \\\n",
      "0     0.516818  0.542725  0.311245  ...  0.423371  0.563632  0.450192   \n",
      "1     0.581968  0.424439  0.325354  ...  0.500776  0.686939  0.409488   \n",
      "2     0.499475  0.478297  0.304691  ...  0.455796  0.508393  0.362873   \n",
      "3     0.676855  0.598703  0.291979  ...  0.486038  0.619912  0.418440   \n",
      "4     0.498685  0.519155  0.290950  ...  0.546421  0.531619  0.430732   \n",
      "...        ...       ...       ...  ...       ...       ...       ...   \n",
      "4410  0.556632  0.654275  0.335470  ...  0.451733  0.608850  0.646362   \n",
      "4411  0.541263  0.665689  0.332891  ...  0.440026  0.610696  0.648783   \n",
      "4412  0.581695  0.661148  0.351011  ...  0.556812  0.646417  0.376303   \n",
      "4413  0.544655  0.446183  0.347874  ...  0.474952  0.584611  0.465631   \n",
      "4414  0.524206  0.511217  0.278563  ...  0.558876  0.657283  0.288699   \n",
      "\n",
      "            15        16        17        18        19  cluster_ID  labels  \n",
      "0     0.443472  0.812926  0.504907  0.590670  0.502805           0     C13  \n",
      "1     0.460807  0.828263  0.469771  0.673681  0.464204           0     C13  \n",
      "2     0.538240  0.800563  0.521545  0.683404  0.572872           0     C13  \n",
      "3     0.415928  0.802655  0.390548  0.695630  0.416554           0     E11  \n",
      "4     0.496672  0.827975  0.414450  0.558520  0.511881           0     C24  \n",
      "...        ...       ...       ...       ...       ...         ...     ...  \n",
      "4410  0.528140  0.700513  0.605510  0.582531  0.572926           0     M12  \n",
      "4411  0.529207  0.697417  0.628885  0.573638  0.583774           0     M12  \n",
      "4412  0.393447  0.806119  0.359862  0.670100  0.476048           0    GCAT  \n",
      "4413  0.466089  0.828018  0.486394  0.636823  0.470072           0    GCAT  \n",
      "4414  0.529782  0.810277  0.366790  0.725083  0.579753           0    C183  \n",
      "\n",
      "[4415 rows x 22 columns],              0         1         2         3         4         5         6  \\\n",
      "0     0.573926  0.580899  0.742427  0.582598  0.406217  0.337102  0.503216   \n",
      "1     0.352730  0.462810  0.913518  0.585133  0.484676  0.212898  0.616677   \n",
      "2     0.556434  0.243002  0.725104  0.630962  0.439220  0.232985  0.481067   \n",
      "3     0.607128  0.453641  0.619393  0.604185  0.318255  0.412688  0.483544   \n",
      "4     0.590411  0.497095  0.589232  0.625312  0.284512  0.416397  0.476649   \n",
      "...        ...       ...       ...       ...       ...       ...       ...   \n",
      "6260  0.586190  0.461494  0.701524  0.675089  0.296680  0.255440  0.377623   \n",
      "6261  0.627765  0.329359  0.656801  0.622263  0.204493  0.408120  0.594829   \n",
      "6262  0.524308  0.374829  0.791934  0.673042  0.458747  0.340489  0.507458   \n",
      "6263  0.425115  0.416920  0.819551  0.717640  0.450484  0.244711  0.627824   \n",
      "6264  0.509772  0.474083  0.768838  0.660341  0.419696  0.101100  0.650157   \n",
      "\n",
      "             7         8         9  ...        12        13        14  \\\n",
      "0     0.539637  0.598794  0.433128  ...  0.334368  0.513383  0.654826   \n",
      "1     0.504363  0.629586  0.480456  ...  0.363674  0.502699  0.350891   \n",
      "2     0.506447  0.521123  0.319987  ...  0.474794  0.590674  0.425993   \n",
      "3     0.652130  0.665090  0.332254  ...  0.414467  0.734828  0.522333   \n",
      "4     0.679097  0.673636  0.337364  ...  0.423320  0.715245  0.518090   \n",
      "...        ...       ...       ...  ...       ...       ...       ...   \n",
      "6260  0.720575  0.574696  0.275237  ...  0.573412  0.549885  0.612220   \n",
      "6261  0.696536  0.598224  0.315545  ...  0.501356  0.646266  0.405319   \n",
      "6262  0.540704  0.693686  0.315398  ...  0.537466  0.508687  0.438079   \n",
      "6263  0.451705  0.651696  0.280807  ...  0.529902  0.546719  0.291571   \n",
      "6264  0.531587  0.687772  0.295622  ...  0.509166  0.392136  0.416654   \n",
      "\n",
      "            15        16        17        18        19  cluster_ID  labels  \n",
      "0     0.306742  0.634377  0.495290  0.530164  0.508667           1     M12  \n",
      "1     0.438738  0.647702  0.355280  0.427002  0.613619           1     E71  \n",
      "2     0.569902  0.812227  0.499429  0.564566  0.652306           1     C31  \n",
      "3     0.512440  0.711829  0.443459  0.653504  0.662395           1     M12  \n",
      "4     0.499547  0.715073  0.422498  0.660429  0.692643           1     M12  \n",
      "...        ...       ...       ...       ...       ...         ...     ...  \n",
      "6260  0.419139  0.703763  0.435416  0.549246  0.682379           1     M13  \n",
      "6261  0.459459  0.754845  0.434467  0.690064  0.554769           1     M13  \n",
      "6262  0.423569  0.793810  0.474455  0.628617  0.529272           1    GDIP  \n",
      "6263  0.423582  0.800477  0.430420  0.520204  0.601215           1    E512  \n",
      "6264  0.364733  0.781316  0.446352  0.624830  0.628587           1     E11  \n",
      "\n",
      "[6265 rows x 22 columns],              0         1         2         3         4         5         6  \\\n",
      "0     0.616497  0.534946  0.758498  0.615259  0.459040  0.573435  0.523550   \n",
      "1     0.680921  0.506620  0.725481  0.668478  0.411418  0.432218  0.556964   \n",
      "2     0.625572  0.594332  0.678646  0.804450  0.344000  0.449821  0.405495   \n",
      "3     0.603871  0.475515  0.647818  0.661845  0.245529  0.369311  0.541645   \n",
      "4     0.712225  0.468530  0.725597  0.610921  0.510333  0.430744  0.468921   \n",
      "...        ...       ...       ...       ...       ...       ...       ...   \n",
      "6248  0.845454  0.499859  0.688799  0.683199  0.268462  0.288435  0.396896   \n",
      "6249  0.691639  0.456826  0.710278  0.617426  0.394864  0.236971  0.693356   \n",
      "6250  0.566580  0.552564  0.580428  0.664786  0.425523  0.346878  0.608785   \n",
      "6251  0.630667  0.596153  0.673406  0.703188  0.310843  0.394795  0.452108   \n",
      "6252  0.669765  0.602131  0.635579  0.622503  0.320627  0.479017  0.389339   \n",
      "\n",
      "             7         8         9  ...        12        13        14  \\\n",
      "0     0.609815  0.452385  0.073795  ...  0.457242  0.505725  0.269170   \n",
      "1     0.723188  0.490364  0.259659  ...  0.464800  0.615725  0.265165   \n",
      "2     0.652300  0.556716  0.339648  ...  0.539671  0.495248  0.355438   \n",
      "3     0.705059  0.481495  0.220468  ...  0.566189  0.672276  0.361509   \n",
      "4     0.534760  0.372489  0.197843  ...  0.455589  0.417022  0.323919   \n",
      "...        ...       ...       ...  ...       ...       ...       ...   \n",
      "6248  0.572045  0.383132  0.287058  ...  0.428887  0.600151  0.244947   \n",
      "6249  0.505573  0.504993  0.230959  ...  0.491050  0.480651  0.194008   \n",
      "6250  0.568099  0.385074  0.292869  ...  0.452063  0.553549  0.210417   \n",
      "6251  0.589253  0.445856  0.252070  ...  0.295953  0.581567  0.386515   \n",
      "6252  0.651892  0.371536  0.163195  ...  0.290003  0.573910  0.284626   \n",
      "\n",
      "            15        16        17        18        19  cluster_ID  labels  \n",
      "0     0.291784  0.765171  0.501073  0.469286  0.663152           2    C181  \n",
      "1     0.479246  0.773312  0.256694  0.477097  0.636108           2    C173  \n",
      "2     0.306046  0.779521  0.330574  0.670101  0.551333           2     C33  \n",
      "3     0.320675  0.773827  0.411471  0.493560  0.638202           2     C18  \n",
      "4     0.370145  0.794029  0.442433  0.523919  0.569821           2     C13  \n",
      "...        ...       ...       ...       ...       ...         ...     ...  \n",
      "6248  0.465095  0.742998  0.337153  0.719662  0.348633           2     C12  \n",
      "6249  0.425779  0.724841  0.425697  0.697856  0.619989           2     C13  \n",
      "6250  0.536400  0.833031  0.483590  0.774110  0.433817           2     C21  \n",
      "6251  0.254488  0.815794  0.443469  0.637748  0.522188           2    C171  \n",
      "6252  0.331974  0.802149  0.426618  0.662631  0.615255           2     C31  \n",
      "\n",
      "[6253 rows x 22 columns],              0         1         2         3         4         5         6  \\\n",
      "0     0.553312  0.366327  0.727912  0.771484  0.460091  0.494093  0.542318   \n",
      "1     0.492046  0.388995  0.806516  0.685273  0.701618  0.485051  0.622977   \n",
      "2     0.552578  0.313691  0.815514  0.577006  0.686654  0.526049  0.579554   \n",
      "3     0.546409  0.310775  0.699680  0.592485  0.161714  0.296951  0.426889   \n",
      "4     0.448036  0.274245  0.663840  0.703539  0.245976  0.394815  0.446689   \n",
      "...        ...       ...       ...       ...       ...       ...       ...   \n",
      "6013  0.470883  0.439214  0.771151  0.633764  0.320886  0.386776  0.365454   \n",
      "6014  0.602427  0.391054  0.716768  0.650235  0.228124  0.345608  0.483953   \n",
      "6015  0.502142  0.421574  0.734434  0.625908  0.258793  0.420332  0.341979   \n",
      "6016  0.496029  0.474137  0.716669  0.644541  0.249470  0.388899  0.426448   \n",
      "6017  0.655175  0.344699  0.758743  0.679330  0.375062  0.457441  0.456382   \n",
      "\n",
      "             7         8         9  ...        12        13        14  \\\n",
      "0     0.530476  0.424093  0.495977  ...  0.339908  0.479551  0.487143   \n",
      "1     0.210947  0.242266  0.534482  ...  0.344672  0.870751  0.286068   \n",
      "2     0.331055  0.316833  0.444708  ...  0.419853  0.788420  0.253307   \n",
      "3     0.632864  0.468281  0.349078  ...  0.514926  0.649965  0.407769   \n",
      "4     0.628682  0.400887  0.425090  ...  0.527768  0.602656  0.353652   \n",
      "...        ...       ...       ...  ...       ...       ...       ...   \n",
      "6013  0.730872  0.510910  0.357651  ...  0.304269  0.684393  0.515349   \n",
      "6014  0.474840  0.545296  0.371464  ...  0.426357  0.706826  0.364642   \n",
      "6015  0.617336  0.528461  0.323932  ...  0.357920  0.577662  0.348478   \n",
      "6016  0.516155  0.573366  0.335308  ...  0.403292  0.599641  0.317118   \n",
      "6017  0.635246  0.430477  0.268273  ...  0.392043  0.641913  0.353857   \n",
      "\n",
      "            15        16        17        18        19  cluster_ID  labels  \n",
      "0     0.468361  0.743514  0.586112  0.366103  0.496401           3     M14  \n",
      "1     0.164795  0.670447  0.806185  0.389510  0.601433           3     C15  \n",
      "2     0.156670  0.705544  0.690352  0.528899  0.629300           3     C15  \n",
      "3     0.443882  0.771610  0.425231  0.636393  0.538887           3     C15  \n",
      "4     0.390670  0.802432  0.365670  0.567793  0.546042           3     C15  \n",
      "...        ...       ...       ...       ...       ...         ...     ...  \n",
      "6013  0.399385  0.750475  0.464848  0.562423  0.487326           3     C15  \n",
      "6014  0.411143  0.751105  0.366685  0.534987  0.581905           3     C15  \n",
      "6015  0.364370  0.756064  0.359511  0.637275  0.509727           3     C15  \n",
      "6016  0.437131  0.730023  0.332298  0.533369  0.461820           3     C15  \n",
      "6017  0.370540  0.751617  0.389456  0.628023  0.597682           3    C151  \n",
      "\n",
      "[6018 rows x 22 columns],              0         1         2         3         4         5         6  \\\n",
      "0     0.424503  0.513149  0.743541  0.565117  0.235157  0.186018  0.299606   \n",
      "1     0.428470  0.581536  0.701913  0.617085  0.265833  0.206637  0.311812   \n",
      "2     0.444713  0.510763  0.722401  0.655193  0.267711  0.251344  0.466389   \n",
      "3     0.349038  0.392062  0.796304  0.646279  0.315626  0.275647  0.496882   \n",
      "4     0.417756  0.466144  0.645099  0.515375  0.331683  0.285184  0.409476   \n",
      "...        ...       ...       ...       ...       ...       ...       ...   \n",
      "4571  0.412649  0.627778  0.643249  0.617745  0.352403  0.261927  0.586677   \n",
      "4572  0.474770  0.458303  0.822320  0.592410  0.343822  0.216421  0.437058   \n",
      "4573  0.439763  0.463515  0.765851  0.693236  0.357087  0.339472  0.486259   \n",
      "4574  0.468500  0.397303  0.788368  0.671508  0.280889  0.445550  0.416132   \n",
      "4575  0.471788  0.472866  0.772790  0.696924  0.321173  0.276746  0.358078   \n",
      "\n",
      "             7         8         9  ...        12        13        14  \\\n",
      "0     0.754168  0.509817  0.280270  ...  0.460574  0.640806  0.474639   \n",
      "1     0.780421  0.470207  0.297177  ...  0.469646  0.631364  0.399221   \n",
      "2     0.734567  0.558805  0.262186  ...  0.389383  0.623160  0.543022   \n",
      "3     0.719967  0.648140  0.183426  ...  0.416530  0.513413  0.509376   \n",
      "4     0.805568  0.529288  0.268806  ...  0.492258  0.669360  0.475197   \n",
      "...        ...       ...       ...  ...       ...       ...       ...   \n",
      "4571  0.674188  0.675706  0.412887  ...  0.416848  0.607191  0.599765   \n",
      "4572  0.676015  0.713334  0.238917  ...  0.479043  0.386849  0.594005   \n",
      "4573  0.643067  0.549261  0.193272  ...  0.381392  0.581139  0.485915   \n",
      "4574  0.780410  0.557540  0.202051  ...  0.403205  0.550593  0.343594   \n",
      "4575  0.746608  0.531727  0.298449  ...  0.442478  0.526387  0.495280   \n",
      "\n",
      "            15        16        17        18        19  cluster_ID  labels  \n",
      "0     0.431265  0.716188  0.515832  0.754109  0.497919           4     M11  \n",
      "1     0.425568  0.747533  0.434324  0.791942  0.548512           4    G152  \n",
      "2     0.346115  0.686646  0.552763  0.713745  0.431787           4     M11  \n",
      "3     0.280150  0.652973  0.644534  0.656658  0.573413           4     M11  \n",
      "4     0.457772  0.747887  0.546047  0.726527  0.668624           4     M11  \n",
      "...        ...       ...       ...       ...       ...         ...     ...  \n",
      "4571  0.366473  0.734626  0.451614  0.678565  0.424944           4     C15  \n",
      "4572  0.376847  0.664370  0.491363  0.565333  0.634067           4     M13  \n",
      "4573  0.295512  0.681089  0.603058  0.609044  0.563284           4     M11  \n",
      "4574  0.317674  0.704506  0.438694  0.684080  0.468132           4     M11  \n",
      "4575  0.349397  0.733620  0.405843  0.687172  0.444415           4     M11  \n",
      "\n",
      "[4576 rows x 22 columns],              0         1         2         3         4         5         6  \\\n",
      "0     0.560244  0.411299  0.751110  0.742309  0.425377  0.332353  0.614625   \n",
      "1     0.593493  0.734288  0.597807  0.818282  0.383758  0.342397  0.457072   \n",
      "2     0.751897  0.532756  0.664164  0.830153  0.299001  0.570125  0.459460   \n",
      "3     0.645990  0.549480  0.614495  0.719774  0.371768  0.379179  0.541939   \n",
      "4     0.656564  0.534468  0.682627  0.732531  0.156424  0.326713  0.241795   \n",
      "...        ...       ...       ...       ...       ...       ...       ...   \n",
      "5163  0.563096  0.354082  0.745350  0.791701  0.290983  0.339514  0.346540   \n",
      "5164  0.732281  0.345458  0.551806  0.778343  0.326297  0.500228  0.362192   \n",
      "5165  0.598091  0.392370  0.708841  0.835158  0.291866  0.409341  0.384028   \n",
      "5166  0.647724  0.317654  0.714157  0.806476  0.341769  0.549437  0.196168   \n",
      "5167  0.482238  0.348528  0.793304  0.846178  0.351244  0.572188  0.269090   \n",
      "\n",
      "             7         8         9  ...        12        13        14  \\\n",
      "0     0.399864  0.670180  0.144894  ...  0.495397  0.571729  0.525861   \n",
      "1     0.676026  0.571516  0.137741  ...  0.498250  0.416266  0.496737   \n",
      "2     0.703992  0.604949  0.189002  ...  0.488465  0.455079  0.377781   \n",
      "3     0.501163  0.750360  0.241654  ...  0.540931  0.661379  0.650292   \n",
      "4     0.700198  0.633734  0.264980  ...  0.441520  0.672824  0.572763   \n",
      "...        ...       ...       ...  ...       ...       ...       ...   \n",
      "5163  0.453847  0.609304  0.354053  ...  0.449807  0.500116  0.377005   \n",
      "5164  0.641739  0.619626  0.184341  ...  0.475867  0.607667  0.545295   \n",
      "5165  0.412313  0.576249  0.261773  ...  0.415330  0.597085  0.343475   \n",
      "5166  0.576512  0.542578  0.268593  ...  0.458723  0.508487  0.441246   \n",
      "5167  0.493922  0.646102  0.309460  ...  0.418347  0.518614  0.334417   \n",
      "\n",
      "            15        16        17        18        19  cluster_ID  labels  \n",
      "0     0.352763  0.728974  0.606749  0.591573  0.417763           5    GCAT  \n",
      "1     0.471168  0.758977  0.587106  0.554372  0.516421           5    GCAT  \n",
      "2     0.422867  0.747849  0.381714  0.614897  0.478422           5    GCAT  \n",
      "3     0.398178  0.722844  0.631772  0.578483  0.401971           5    GCAT  \n",
      "4     0.352693  0.807703  0.471009  0.478120  0.257538           5    GCAT  \n",
      "...        ...       ...       ...       ...       ...         ...     ...  \n",
      "5163  0.553455  0.806496  0.452502  0.568664  0.486904           5    GCAT  \n",
      "5164  0.455632  0.749208  0.580755  0.482634  0.462109           5    GCAT  \n",
      "5165  0.454675  0.756337  0.481191  0.603503  0.512131           5    GCAT  \n",
      "5166  0.469470  0.795006  0.530541  0.559399  0.496134           5    GCAT  \n",
      "5167  0.510463  0.804065  0.573654  0.660650  0.412199           5    GCAT  \n",
      "\n",
      "[5168 rows x 22 columns],              0         1         2         3         4         5         6  \\\n",
      "0     0.500216  0.360693  0.550247  0.464599  0.274903  0.653536  0.177024   \n",
      "1     0.482071  0.316059  0.619495  0.520908  0.283757  0.642519  0.239165   \n",
      "2     0.432538  0.572998  0.738977  0.580037  0.454497  0.681278  0.225442   \n",
      "3     0.418392  0.474869  0.682791  0.604454  0.377467  0.508674  0.336939   \n",
      "4     0.661012  0.468814  0.557614  0.628562  0.386920  0.423010  0.254443   \n",
      "...        ...       ...       ...       ...       ...       ...       ...   \n",
      "2409  0.690954  0.477472  0.551921  0.676410  0.478165  0.472573  0.485094   \n",
      "2410  0.401776  0.684229  0.541646  0.786175  0.505387  0.759171  0.368737   \n",
      "2411  0.654470  0.394467  0.646233  0.692781  0.260112  0.359471  0.260129   \n",
      "2412  0.495964  0.413145  0.688077  0.713565  0.335128  0.378570  0.500032   \n",
      "2413  0.410672  0.393893  0.682518  0.676442  0.283567  0.429817  0.244019   \n",
      "\n",
      "             7         8         9  ...        12        13        14  \\\n",
      "0     0.667071  0.460599  0.214179  ...  0.545139  0.606755  0.647157   \n",
      "1     0.639243  0.474147  0.222134  ...  0.557003  0.668906  0.588077   \n",
      "2     0.513619  0.586815  0.462584  ...  0.711541  0.597903  0.658424   \n",
      "3     0.595900  0.615082  0.323955  ...  0.596616  0.610776  0.599582   \n",
      "4     0.580734  0.538005  0.352571  ...  0.553320  0.449594  0.566568   \n",
      "...        ...       ...       ...  ...       ...       ...       ...   \n",
      "2409  0.672860  0.495105  0.442699  ...  0.482094  0.452022  0.666470   \n",
      "2410  0.616900  0.403177  0.482813  ...  0.442503  0.344157  0.505161   \n",
      "2411  0.687847  0.554343  0.301822  ...  0.432096  0.750688  0.660837   \n",
      "2412  0.719801  0.575857  0.305431  ...  0.483903  0.713124  0.555323   \n",
      "2413  0.691750  0.621461  0.336923  ...  0.543945  0.693417  0.513325   \n",
      "\n",
      "            15        16        17        18        19  cluster_ID  labels  \n",
      "0     0.394229  0.790418  0.649700  0.561055  0.167905           6    GCAT  \n",
      "1     0.392197  0.784524  0.622649  0.560548  0.213207           6    GCAT  \n",
      "2     0.346442  0.775047  0.378205  0.601274  0.418708           6    GCAT  \n",
      "3     0.328601  0.806031  0.468524  0.701798  0.224919           6    GCAT  \n",
      "4     0.427692  0.796810  0.492062  0.723642  0.370942           6    GCAT  \n",
      "...        ...       ...       ...       ...       ...         ...     ...  \n",
      "2409  0.380079  0.754828  0.469873  0.591385  0.483259           6     C24  \n",
      "2410  0.447723  0.756432  0.617918  0.520482  0.270530           6    CCAT  \n",
      "2411  0.468630  0.806736  0.569368  0.540852  0.315257           6    GCAT  \n",
      "2412  0.460016  0.859614  0.576246  0.580945  0.166241           6    GCAT  \n",
      "2413  0.464840  0.846261  0.485367  0.619913  0.165567           6    GCAT  \n",
      "\n",
      "[2414 rows x 22 columns],              0         1         2         3         4         5         6  \\\n",
      "0     0.438419  0.341882  0.703434  0.608451  0.458451  0.399186  0.479678   \n",
      "1     0.369499  0.392484  0.664244  0.722163  0.552684  0.577181  0.447102   \n",
      "2     0.404544  0.350839  0.700166  0.664606  0.510197  0.423745  0.512761   \n",
      "3     0.474491  0.384005  0.715226  0.618930  0.489664  0.392297  0.458918   \n",
      "4     0.482285  0.366689  0.737903  0.623221  0.481582  0.428891  0.380103   \n",
      "...        ...       ...       ...       ...       ...       ...       ...   \n",
      "4319  0.338629  0.362948  0.774189  0.650256  0.414628  0.446109  0.653467   \n",
      "4320  0.555482  0.322689  0.734212  0.679478  0.398120  0.348525  0.470878   \n",
      "4321  0.623211  0.298005  0.693951  0.774968  0.379918  0.599619  0.378755   \n",
      "4322  0.620613  0.348458  0.639821  0.716438  0.457043  0.344323  0.464294   \n",
      "4323  0.499215  0.477484  0.810592  0.659651  0.518665  0.498898  0.506445   \n",
      "\n",
      "             7         8         9  ...        12        13        14  \\\n",
      "0     0.825788  0.474819  0.257009  ...  0.367373  0.421284  0.429893   \n",
      "1     0.570698  0.476386  0.494781  ...  0.306322  0.551534  0.346597   \n",
      "2     0.729589  0.654702  0.447153  ...  0.421849  0.544242  0.457816   \n",
      "3     0.644931  0.618676  0.443908  ...  0.404967  0.494108  0.444273   \n",
      "4     0.619750  0.599789  0.420203  ...  0.416122  0.475733  0.443445   \n",
      "...        ...       ...       ...  ...       ...       ...       ...   \n",
      "4319  0.577207  0.479769  0.480059  ...  0.353581  0.504129  0.348689   \n",
      "4320  0.669633  0.549880  0.324442  ...  0.398746  0.536680  0.304585   \n",
      "4321  0.736253  0.570300  0.329582  ...  0.389513  0.421503  0.401102   \n",
      "4322  0.679096  0.439016  0.451664  ...  0.413065  0.515075  0.593115   \n",
      "4323  0.498049  0.512586  0.295727  ...  0.402693  0.341776  0.257474   \n",
      "\n",
      "            15        16        17        18        19  cluster_ID  labels  \n",
      "0     0.446037  0.773608  0.450486  0.676226  0.472393           7     M14  \n",
      "1     0.285553  0.593309  0.620889  0.545628  0.533144           7     M11  \n",
      "2     0.413031  0.726687  0.493716  0.396239  0.601314           7     M14  \n",
      "3     0.449175  0.717294  0.501356  0.342237  0.643686           7     M14  \n",
      "4     0.400277  0.729348  0.468209  0.372799  0.639143           7     M14  \n",
      "...        ...       ...       ...       ...       ...         ...     ...  \n",
      "4319  0.544443  0.768755  0.397316  0.718874  0.424489           7     C31  \n",
      "4320  0.514065  0.751026  0.320595  0.629725  0.647584           7     M14  \n",
      "4321  0.369512  0.756299  0.393496  0.610097  0.615095           7     M14  \n",
      "4322  0.438247  0.796787  0.431384  0.596403  0.493846           7     C24  \n",
      "4323  0.425478  0.808252  0.335245  0.656890  0.563238           7     C31  \n",
      "\n",
      "[4324 rows x 22 columns],              0         1         2         3         4         5         6  \\\n",
      "0     0.638959  0.476085  0.694580  0.744784  0.372017  0.387336  0.524268   \n",
      "1     0.794760  0.503550  0.651368  0.602274  0.434045  0.380334  0.484611   \n",
      "2     0.756503  0.450862  0.651098  0.648509  0.285184  0.316422  0.397211   \n",
      "3     0.760770  0.508678  0.585668  0.626022  0.437624  0.414515  0.499447   \n",
      "4     0.695821  0.535141  0.728804  0.731425  0.367532  0.305556  0.415406   \n",
      "...        ...       ...       ...       ...       ...       ...       ...   \n",
      "3566  0.603890  0.522198  0.684361  0.651138  0.307824  0.306096  0.276603   \n",
      "3567  0.649401  0.701299  0.656345  0.712994  0.228307  0.246548  0.421750   \n",
      "3568  0.785940  0.355372  0.664386  0.758839  0.471250  0.501290  0.506613   \n",
      "3569  0.635205  0.513025  0.789718  0.742609  0.368192  0.250265  0.553794   \n",
      "3570  0.660082  0.333027  0.732531  0.745563  0.282386  0.344302  0.511839   \n",
      "\n",
      "             7         8         9  ...        12        13        14  \\\n",
      "0     0.713396  0.559258  0.327543  ...  0.354652  0.594333  0.558034   \n",
      "1     0.569001  0.489638  0.343252  ...  0.269092  0.476857  0.393608   \n",
      "2     0.525311  0.378770  0.232827  ...  0.290321  0.730764  0.361933   \n",
      "3     0.569415  0.619761  0.295485  ...  0.313353  0.542696  0.458852   \n",
      "4     0.517857  0.490153  0.324742  ...  0.348725  0.659332  0.456827   \n",
      "...        ...       ...       ...  ...       ...       ...       ...   \n",
      "3566  0.718746  0.313087  0.165610  ...  0.282858  0.764798  0.366022   \n",
      "3567  0.602269  0.564830  0.294866  ...  0.365380  0.739564  0.521059   \n",
      "3568  0.645527  0.512896  0.152333  ...  0.442447  0.486527  0.518044   \n",
      "3569  0.620913  0.556688  0.347471  ...  0.348228  0.479559  0.504001   \n",
      "3570  0.576362  0.683088  0.256293  ...  0.427727  0.714969  0.646462   \n",
      "\n",
      "            15        16        17        18        19  cluster_ID  labels  \n",
      "0     0.435491  0.802828  0.447532  0.546236  0.288549           8     C32  \n",
      "1     0.336897  0.833914  0.335029  0.537473  0.258438           8     C23  \n",
      "2     0.408226  0.750443  0.518122  0.628091  0.468948           8     C12  \n",
      "3     0.304477  0.843346  0.372856  0.575777  0.282285           8     C23  \n",
      "4     0.364216  0.746546  0.502150  0.487658  0.498138           8     C12  \n",
      "...        ...       ...       ...       ...       ...         ...     ...  \n",
      "3566  0.356666  0.830389  0.669989  0.568244  0.292994           8     C12  \n",
      "3567  0.444752  0.767999  0.516629  0.624527  0.341800           8     C12  \n",
      "3568  0.430462  0.812029  0.572373  0.482259  0.318975           8    GCAT  \n",
      "3569  0.500807  0.751756  0.455004  0.656763  0.361759           8    GCAT  \n",
      "3570  0.472763  0.784795  0.608744  0.515793  0.242777           8    GCAT  \n",
      "\n",
      "[3571 rows x 22 columns],              0         1         2         3         4         5         6  \\\n",
      "0     0.498789  0.520745  0.702054  0.565256  0.191545  0.284360  0.155971   \n",
      "1     0.595084  0.570282  0.586857  0.606866  0.259046  0.327588  0.412268   \n",
      "2     0.484037  0.319405  0.720507  0.648207  0.301180  0.351847  0.373639   \n",
      "3     0.544435  0.523063  0.654865  0.621562  0.391120  0.269768  0.277232   \n",
      "4     0.559772  0.467282  0.677459  0.578584  0.199753  0.255534  0.335700   \n",
      "...        ...       ...       ...       ...       ...       ...       ...   \n",
      "5248  0.599146  0.446013  0.673495  0.693155  0.341129  0.358333  0.435542   \n",
      "5249  0.547875  0.371535  0.705710  0.762970  0.276819  0.368599  0.236340   \n",
      "5250  0.484727  0.311620  0.778543  0.643622  0.409879  0.475207  0.434067   \n",
      "5251  0.597560  0.531167  0.642696  0.649109  0.204281  0.394406  0.356510   \n",
      "5252  0.435629  0.500447  0.785984  0.640041  0.459527  0.483946  0.428015   \n",
      "\n",
      "             7         8         9  ...        12        13        14  \\\n",
      "0     0.668012  0.546567  0.236495  ...  0.517391  0.648904  0.284838   \n",
      "1     0.842820  0.476343  0.334984  ...  0.576945  0.584942  0.374724   \n",
      "2     0.608150  0.610169  0.321361  ...  0.524114  0.682756  0.337865   \n",
      "3     0.729513  0.410728  0.358510  ...  0.470275  0.454049  0.363027   \n",
      "4     0.665978  0.669332  0.368933  ...  0.585444  0.621835  0.435560   \n",
      "...        ...       ...       ...  ...       ...       ...       ...   \n",
      "5248  0.716089  0.603246  0.316030  ...  0.467016  0.598885  0.398830   \n",
      "5249  0.573521  0.509798  0.330233  ...  0.492698  0.633347  0.396273   \n",
      "5250  0.406852  0.539516  0.330238  ...  0.544958  0.541072  0.162541   \n",
      "5251  0.727102  0.481218  0.267793  ...  0.496905  0.682132  0.343141   \n",
      "5252  0.647103  0.527925  0.229324  ...  0.516410  0.383579  0.208501   \n",
      "\n",
      "            15        16        17        18        19  cluster_ID  labels  \n",
      "0     0.318974  0.780804  0.397508  0.682614  0.519343           9     C18  \n",
      "1     0.391079  0.878290  0.303348  0.692264  0.438013           9     C11  \n",
      "2     0.365448  0.783826  0.384331  0.667135  0.618852           9     C17  \n",
      "3     0.456543  0.858760  0.366070  0.733025  0.544454           9     C31  \n",
      "4     0.331485  0.811183  0.325852  0.633183  0.462079           9     E12  \n",
      "...        ...       ...       ...       ...       ...         ...     ...  \n",
      "5248  0.417331  0.761292  0.350580  0.667119  0.548054           9     M11  \n",
      "5249  0.323492  0.782745  0.439525  0.666716  0.637986           9     C14  \n",
      "5250  0.360928  0.827911  0.342491  0.679767  0.693371           9     C15  \n",
      "5251  0.334571  0.769301  0.377450  0.653490  0.549439           9     C33  \n",
      "5252  0.211038  0.823398  0.279534  0.587086  0.494817           9    C152  \n",
      "\n",
      "[5253 rows x 22 columns]]\n"
     ]
    }
   ],
   "source": [
    "#Here Auto encoder is used for feature extraction.\n",
    "#Autoencoder is an unsupervised artificial neural network that learns how to efficiently compress and encode data then learns how to reconstruct the data back from the reduced encoded representation to a representation that is as close to the original input as possible.\n",
    "#Autoencoder, by design, reduces data dimensions by learning how to ignore the noise in the data.\n",
    "#It consists of 4 parts , one is encoder in which model learns how to reduce the input dimensions and compress the input data into encoded representation.\n",
    "#Bottle neck , this layer contains the compressed representation of nput data\n",
    "#Decoder , in which model learns how to recontruct the data from encoded representation to be close to the original input as possible.\n",
    "#I decided to use ReLu as the activation function for the encoding stage and Softmax for the decoding stage.\n",
    "#In here I have declared 3 hidden layers in the encoded stage and 3 hidden layers in the decoded stage.\n",
    "\n",
    "def enhancedFeatureExtraction(receivedFeatureData):\n",
    "    print(\"Enhancing Features using AutoEncoder..........\")\n",
    "    x = Input(shape=(receivedFeatureData.shape[1],))\n",
    "    # 3 hidden layers are implemented\n",
    "    hidden_1en = Dense(2048, activation='relu')(x)\n",
    "    hidden_2en = Dense(1024, activation='relu')(hidden_1en)\n",
    "    hidden_3en = Dense(512, activation='relu')(hidden_2en)\n",
    "    h = Dense(128, activation='relu')(hidden_3en)\n",
    "    hidden_1dec = Dense(512, activation='relu')(h)\n",
    "    hidden_2dec = Dense(1024, activation='relu')(hidden_1dec)\n",
    "    hidden_3dec = Dense(2048, activation='relu')(hidden_2dec)\n",
    "    r = Dense(receivedFeatureData.shape[1], activation='sigmoid')(hidden_3dec)\n",
    "    autoencoder = Model(x, r)\n",
    "    autoencoder.compile(optimizer='adam', loss='mse')\n",
    "    Xtraut, Xteaut, Ytraut, Yteaut = train_test_split(receivedFeatureData, receivedFeatureData, test_size=0.3, random_state=101)\n",
    "    autoencoder.fit(Xtraut, Ytraut,\n",
    "                    epochs=30,\n",
    "                    batch_size=200,\n",
    "                    shuffle=True,\n",
    "                    verbose=0,\n",
    "                    validation_data=(Xteaut, Yteaut))\n",
    "    compressedData = autoencoder.predict(receivedFeatureData)\n",
    "    return compressedData\n",
    "\n",
    "for lf in clusterDataframeList:\n",
    " enhancedDFrame = lf.iloc[:, :-2]\n",
    " clusterandLabel = lf.iloc[:, -2:]\n",
    " compressedFrame = enhancedFeatureExtraction(enhancedDFrame) # PERFORMING AUTO ENCODER FOR DIFFERENT CLUSTER DATA\n",
    " compressedDataFrame = pd.DataFrame(data=compressedFrame)\n",
    " clusterandLabel.reset_index(drop=True, inplace=True)\n",
    " compressedDataFrame.reset_index(drop=True, inplace=True)\n",
    " compressedDataFrame = pd.concat([compressedDataFrame,clusterandLabel],axis=1)\n",
    " enhancedDFList.append(compressedDataFrame) \n",
    "    \n",
    "print(enhancedDFList)# PRINTING THE EXTRACTED FEATURES USING AUTO ENCODER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### APPLYING CLASSIFIERS AFTER FEATURE EXTRACTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Networks\n",
      "========================================\n",
      "Accuracy :  0.5662514156285391\n",
      "Classification Report :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         C11       0.00      0.00      0.00         0\n",
      "         C12       0.67      0.33      0.44         6\n",
      "         C13       0.48      0.48      0.48        75\n",
      "         C14       0.00      0.00      0.00         0\n",
      "         C15       0.17      0.08      0.11        13\n",
      "        C151       0.00      0.00      0.00         0\n",
      "         C16       0.00      0.00      0.00         1\n",
      "         C17       0.00      0.00      0.00         1\n",
      "         C18       0.00      0.00      0.00         6\n",
      "        C183       0.00      0.00      0.00         0\n",
      "         C21       0.14      0.17      0.15         6\n",
      "         C23       0.00      0.00      0.00         0\n",
      "         C24       0.45      0.45      0.45        11\n",
      "         C31       0.33      0.64      0.44        11\n",
      "         C34       0.00      0.00      0.00         0\n",
      "         C41       0.25      1.00      0.40         1\n",
      "         C42       0.42      0.39      0.41        38\n",
      "        CCAT       0.00      0.00      0.00         0\n",
      "         E11       0.52      0.49      0.50        67\n",
      "         E12       0.61      0.64      0.62       112\n",
      "         E13       0.50      0.41      0.45        51\n",
      "        E131       0.00      0.00      0.00         0\n",
      "         E14       0.12      0.50      0.20         2\n",
      "         E21       0.50      0.47      0.49        57\n",
      "         E31       0.31      0.24      0.27        17\n",
      "         E41       0.24      0.37      0.29        27\n",
      "         E51       0.29      0.37      0.33        19\n",
      "         E61       0.00      0.00      0.00         0\n",
      "         E71       0.89      0.77      0.83        31\n",
      "         G15       0.45      0.69      0.55        13\n",
      "        G151       0.00      0.00      0.00         0\n",
      "        G155       0.00      0.00      0.00         0\n",
      "        GCAT       0.88      0.72      0.79       301\n",
      "        GPOL       0.00      0.00      0.00         0\n",
      "         M11       0.60      0.55      0.57        11\n",
      "         M12       0.20      0.33      0.25         3\n",
      "         M13       0.00      0.00      0.00         3\n",
      "\n",
      "    accuracy                           0.57       883\n",
      "   macro avg       0.24      0.27      0.24       883\n",
      "weighted avg       0.62      0.57      0.59       883\n",
      "\n",
      "SVC\n",
      "========================================\n",
      "Accuracy :  0.16520351157222665\n",
      "Classification Report :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         C11       0.00      0.00      0.00         0\n",
      "         C12       0.00      0.00      0.00         0\n",
      "         C13       0.00      0.00      0.00         0\n",
      "         C14       0.00      0.00      0.00         0\n",
      "         C15       0.00      0.00      0.00         0\n",
      "         C16       0.00      0.00      0.00         0\n",
      "         C17       0.00      0.00      0.00         0\n",
      "         C18       0.00      0.00      0.00         0\n",
      "        C183       0.00      0.00      0.00         0\n",
      "         C21       0.00      0.00      0.00         0\n",
      "         C22       0.00      0.00      0.00         0\n",
      "         C23       0.00      0.00      0.00         0\n",
      "         C24       0.00      0.00      0.00         0\n",
      "         C31       0.00      0.00      0.00         0\n",
      "         C32       0.00      0.00      0.00         0\n",
      "         C33       0.00      0.00      0.00         0\n",
      "         C34       0.00      0.00      0.00         0\n",
      "         C41       0.00      0.00      0.00         0\n",
      "         E11       0.00      0.00      0.00         0\n",
      "         E12       0.00      0.00      0.00         0\n",
      "         E13       0.00      0.00      0.00         0\n",
      "         E14       0.00      0.00      0.00         0\n",
      "         E21       0.00      0.00      0.00         0\n",
      "         E31       0.00      0.00      0.00         0\n",
      "         E41       0.00      0.00      0.00         0\n",
      "         E51       0.00      0.00      0.00         0\n",
      "        E512       0.00      0.00      0.00         0\n",
      "         E61       0.00      0.00      0.00         0\n",
      "         E71       0.00      0.00      0.00         0\n",
      "         G15       0.00      0.00      0.00         0\n",
      "        G152       0.00      0.00      0.00         0\n",
      "        GCAT       0.00      0.00      0.00         0\n",
      "         M11       0.00      0.00      0.00         0\n",
      "         M12       0.00      0.00      0.00         0\n",
      "         M13       1.00      0.17      0.28      1253\n",
      "         M14       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.17      1253\n",
      "   macro avg       0.03      0.00      0.01      1253\n",
      "weighted avg       1.00      0.17      0.28      1253\n",
      "\n",
      "Decision Trees\n",
      "========================================\n",
      "Accuracy :  0.28137490007993604\n",
      "Classification Report :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         C11       0.53      0.18      0.26       370\n",
      "         C12       0.00      0.00      0.00         0\n",
      "         C13       0.00      0.00      0.00         0\n",
      "         C14       0.00      0.00      0.00         0\n",
      "         C15       0.76      0.41      0.53       590\n",
      "        C151       0.00      0.00      0.00         0\n",
      "        C152       0.00      0.00      0.00         0\n",
      "         C16       0.00      0.00      0.00         0\n",
      "         C17       0.00      0.00      0.00         0\n",
      "         C18       0.29      0.16      0.21       291\n",
      "        C181       0.00      0.00      0.00         0\n",
      "         C21       0.00      0.00      0.00         0\n",
      "         C22       0.00      0.00      0.00         0\n",
      "         C23       0.00      0.00      0.00         0\n",
      "         C24       0.00      0.00      0.00         0\n",
      "         C31       0.00      0.00      0.00         0\n",
      "         C32       0.00      0.00      0.00         0\n",
      "         C33       0.00      0.00      0.00         0\n",
      "         C34       0.00      0.00      0.00         0\n",
      "         C41       0.00      0.00      0.00         0\n",
      "         C42       0.00      0.00      0.00         0\n",
      "        CCAT       0.00      0.00      0.00         0\n",
      "         E11       0.00      0.00      0.00         0\n",
      "         E12       0.00      0.00      0.00         0\n",
      "         E13       0.00      0.00      0.00         0\n",
      "         E14       0.00      0.00      0.00         0\n",
      "         E21       0.00      0.00      0.00         0\n",
      "         E51       0.00      0.00      0.00         0\n",
      "        GCAT       0.00      0.00      0.00         0\n",
      "        GDEF       0.00      0.00      0.00         0\n",
      "        GJOB       0.00      0.00      0.00         0\n",
      "         M11       0.00      0.00      0.00         0\n",
      "         M12       0.00      0.00      0.00         0\n",
      "         M13       0.00      0.00      0.00         0\n",
      "         M14       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.28      1251\n",
      "   macro avg       0.05      0.02      0.03      1251\n",
      "weighted avg       0.58      0.28      0.38      1251\n",
      "\n",
      "Random Forest\n",
      "========================================\n",
      "Accuracy :  0.5780730897009967\n",
      "Classification Report :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         C11       0.00      0.00      0.00         0\n",
      "         C12       0.00      0.00      0.00         0\n",
      "         C13       0.00      0.00      0.00         0\n",
      "         C14       0.00      0.00      0.00         0\n",
      "         C15       1.00      0.58      0.73      1204\n",
      "        C151       0.00      0.00      0.00         0\n",
      "        C152       0.00      0.00      0.00         0\n",
      "         C16       0.00      0.00      0.00         0\n",
      "         C17       0.00      0.00      0.00         0\n",
      "         C18       0.00      0.00      0.00         0\n",
      "        C181       0.00      0.00      0.00         0\n",
      "         C21       0.00      0.00      0.00         0\n",
      "         C22       0.00      0.00      0.00         0\n",
      "         C23       0.00      0.00      0.00         0\n",
      "         C24       0.00      0.00      0.00         0\n",
      "         C31       0.00      0.00      0.00         0\n",
      "         C32       0.00      0.00      0.00         0\n",
      "         C33       0.00      0.00      0.00         0\n",
      "         C34       0.00      0.00      0.00         0\n",
      "         C41       0.00      0.00      0.00         0\n",
      "         C42       0.00      0.00      0.00         0\n",
      "         E12       0.00      0.00      0.00         0\n",
      "         E13       0.00      0.00      0.00         0\n",
      "         E14       0.00      0.00      0.00         0\n",
      "         E21       0.00      0.00      0.00         0\n",
      "         E51       0.00      0.00      0.00         0\n",
      "         E71       0.00      0.00      0.00         0\n",
      "         G15       0.00      0.00      0.00         0\n",
      "        GCAT       0.00      0.00      0.00         0\n",
      "         M11       0.00      0.00      0.00         0\n",
      "         M12       0.00      0.00      0.00         0\n",
      "         M13       0.00      0.00      0.00         0\n",
      "         M14       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.58      1204\n",
      "   macro avg       0.03      0.02      0.02      1204\n",
      "weighted avg       1.00      0.58      0.73      1204\n",
      "\n",
      "KNearestNeighbors\n",
      "=========================================\n",
      "Accuracy :  0.7205240174672489\n",
      "Classification Report :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         C11       0.00      0.00      0.00         0\n",
      "         C12       0.00      0.00      0.00         3\n",
      "         C13       0.00      0.00      0.00         2\n",
      "         C14       0.00      0.00      0.00         1\n",
      "         C15       0.67      0.66      0.66        87\n",
      "         C17       0.00      0.00      0.00         1\n",
      "         C18       0.00      0.00      0.00         0\n",
      "         C21       0.00      0.00      0.00         1\n",
      "         C24       0.00      0.00      0.00         0\n",
      "         C31       0.24      0.29      0.26        14\n",
      "         C41       0.00      0.00      0.00         0\n",
      "         E11       0.17      0.11      0.13         9\n",
      "         E12       0.33      0.41      0.37        39\n",
      "         E13       0.00      0.00      0.00         0\n",
      "        E131       0.00      0.00      0.00         0\n",
      "         E14       0.00      0.00      0.00         1\n",
      "         E21       0.00      0.00      0.00         0\n",
      "         E51       0.43      0.75      0.55         4\n",
      "         E71       0.00      0.00      0.00         2\n",
      "         G15       0.00      0.00      0.00         0\n",
      "        G151       0.00      0.00      0.00         0\n",
      "        G154       0.00      0.00      0.00         0\n",
      "        GCAT       0.00      0.00      0.00         0\n",
      "        GENT       0.00      0.00      0.00         0\n",
      "        GPOL       0.00      0.00      0.00         0\n",
      "         M11       0.92      0.80      0.86       406\n",
      "         M12       0.71      0.71      0.71       131\n",
      "         M13       0.79      0.74      0.76       204\n",
      "         M14       0.50      1.00      0.67        11\n",
      "\n",
      "    accuracy                           0.72       916\n",
      "   macro avg       0.16      0.19      0.17       916\n",
      "weighted avg       0.78      0.72      0.75       916\n",
      "\n",
      "Guassian Naive Bayes\n",
      "=========================================\n",
      "Accuracy :  0.7601547388781431\n",
      "Classification Report :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         C11       0.25      0.33      0.29         3\n",
      "         C12       0.71      0.16      0.26        31\n",
      "         C13       0.05      0.07      0.06        14\n",
      "         C15       0.00      0.00      0.00         0\n",
      "         C21       0.00      0.00      0.00         0\n",
      "         C22       0.00      0.00      0.00         2\n",
      "         C24       0.20      0.13      0.16        31\n",
      "         C31       0.33      0.25      0.29         4\n",
      "         C33       0.00      0.00      0.00         1\n",
      "         C41       0.00      0.00      0.00         2\n",
      "         C42       0.25      0.10      0.14        10\n",
      "        CCAT       0.00      0.00      0.00        24\n",
      "         E11       1.00      0.33      0.50         3\n",
      "         E12       0.33      0.20      0.25         5\n",
      "         E21       0.33      0.25      0.29         4\n",
      "         E41       0.00      0.00      0.00         2\n",
      "         E51       0.20      0.26      0.23        31\n",
      "        E512       0.00      0.00      0.00         1\n",
      "        ECAT       1.00      0.50      0.67         2\n",
      "         G15       0.68      0.43      0.53        60\n",
      "        G155       0.00      0.00      0.00         1\n",
      "        GCAT       0.85      0.92      0.88       798\n",
      "        GDIP       0.00      0.00      0.00         0\n",
      "        GDIS       0.00      0.00      0.00         2\n",
      "        GPOL       0.00      0.00      0.00         0\n",
      "        GVIO       0.00      0.00      0.00         1\n",
      "         M14       1.00      0.50      0.67         2\n",
      "\n",
      "    accuracy                           0.76      1034\n",
      "   macro avg       0.27      0.16      0.19      1034\n",
      "weighted avg       0.75      0.76      0.74      1034\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guassian Multinomial Naive Bayes\n",
      "=========================================\n",
      "Accuracy :  0.937888198757764\n",
      "Classification Report :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         C12       0.00      0.00      0.00         0\n",
      "         C15       0.00      0.00      0.00         0\n",
      "         C21       0.00      0.00      0.00         0\n",
      "         C22       0.00      0.00      0.00         0\n",
      "         C24       0.00      0.00      0.00         0\n",
      "        CCAT       0.00      0.00      0.00         0\n",
      "        GCAT       1.00      0.94      0.97       483\n",
      "        GDIP       0.00      0.00      0.00         0\n",
      "         M11       0.00      0.00      0.00         0\n",
      "         M12       0.00      0.00      0.00         0\n",
      "         M13       0.00      0.00      0.00         0\n",
      "         M14       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.94       483\n",
      "   macro avg       0.08      0.08      0.08       483\n",
      "weighted avg       1.00      0.94      0.97       483\n",
      "\n",
      "Stochastic Gradient Descent\n",
      "=========================================\n",
      "Accuracy :  0.7144508670520231\n",
      "Classification Report :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         C11       0.00      0.00      0.00         0\n",
      "         C13       0.03      1.00      0.07         1\n",
      "         C14       0.00      0.00      0.00         0\n",
      "         C15       0.08      0.17      0.11         6\n",
      "         C18       0.00      0.00      0.00         0\n",
      "         C21       0.42      0.55      0.48        65\n",
      "         C22       0.00      0.00      0.00         0\n",
      "         C24       0.30      0.76      0.43        25\n",
      "         C31       0.06      0.80      0.12         5\n",
      "         C33       0.43      0.07      0.12        41\n",
      "         C42       0.00      0.00      0.00         0\n",
      "        CCAT       0.50      0.50      0.50         2\n",
      "         E13       0.00      0.00      0.00         0\n",
      "         E14       0.00      0.00      0.00         0\n",
      "         E51       0.00      0.00      0.00         0\n",
      "        E512       0.00      0.00      0.00         0\n",
      "        GCAT       0.69      0.45      0.55        20\n",
      "         M11       0.00      0.00      0.00         0\n",
      "         M13       0.00      0.00      0.00         0\n",
      "         M14       0.97      0.78      0.86       700\n",
      "        M141       0.00      0.00      0.00         0\n",
      "        MCAT       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.71       865\n",
      "   macro avg       0.16      0.23      0.15       865\n",
      "weighted avg       0.87      0.71      0.77       865\n",
      "\n",
      "ADA-Boost\n",
      "=========================================\n",
      "Accuracy :  0.46013986013986014\n",
      "Classification Report :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         C11       0.00      0.00      0.00         0\n",
      "         C12       0.00      0.00      0.00         0\n",
      "         C13       0.00      0.00      0.00         0\n",
      "         C15       0.00      0.00      0.00         0\n",
      "         C17       0.00      0.00      0.00         0\n",
      "         C18       0.00      0.00      0.00         0\n",
      "         C21       0.00      0.00      0.00         0\n",
      "         C22       0.00      0.00      0.00         0\n",
      "         C23       0.00      0.00      0.00         0\n",
      "         C24       0.00      0.00      0.00         0\n",
      "         C31       0.00      0.00      0.00         0\n",
      "         C32       0.00      0.00      0.00         0\n",
      "         C33       0.00      0.00      0.00         0\n",
      "         C41       0.00      0.00      0.00         0\n",
      "         C42       0.00      0.00      0.00         0\n",
      "        CCAT       0.00      0.00      0.00         0\n",
      "         E11       0.00      0.00      0.00         0\n",
      "         E12       0.00      0.00      0.00         0\n",
      "         E13       0.00      0.00      0.00         0\n",
      "         E21       0.85      0.34      0.49       342\n",
      "         E41       0.00      0.00      0.00         0\n",
      "         E51       0.00      0.00      0.00         0\n",
      "         G15       0.00      0.00      0.00         0\n",
      "        GCAT       0.77      0.57      0.66       373\n",
      "        GPOL       0.00      0.00      0.00         0\n",
      "         M11       0.00      0.00      0.00         0\n",
      "         M12       0.00      0.00      0.00         0\n",
      "         M13       0.00      0.00      0.00         0\n",
      "         M14       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.46       715\n",
      "   macro avg       0.06      0.03      0.04       715\n",
      "weighted avg       0.81      0.46      0.58       715\n",
      "\n",
      "Bagging\n",
      "=========================================\n",
      "Accuracy :  0.5223596574690771\n",
      "Classification Report :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         C11       0.37      0.28      0.32       150\n",
      "         C12       0.88      0.64      0.74        11\n",
      "         C13       0.11      0.38      0.17        13\n",
      "         C14       0.04      0.25      0.07         4\n",
      "         C15       0.89      0.61      0.73       539\n",
      "        C151       0.00      0.00      0.00         1\n",
      "        C152       0.00      0.00      0.00         1\n",
      "         C16       0.00      0.00      0.00         2\n",
      "         C17       0.07      0.33      0.11         9\n",
      "         C18       0.58      0.51      0.54       184\n",
      "        C181       0.00      0.00      0.00         1\n",
      "        C183       0.00      0.00      0.00         0\n",
      "         C21       0.22      0.53      0.31        15\n",
      "         C22       0.33      1.00      0.50         2\n",
      "         C23       0.00      0.00      0.00         0\n",
      "         C24       0.14      0.36      0.20        14\n",
      "         C31       0.23      0.46      0.31        24\n",
      "         C32       0.00      0.00      0.00         0\n",
      "         C33       0.22      0.25      0.24        24\n",
      "         C34       0.00      0.00      0.00         1\n",
      "         C41       0.08      1.00      0.15         1\n",
      "         C42       0.38      0.30      0.33        10\n",
      "        CCAT       0.00      0.00      0.00         0\n",
      "         E11       0.00      0.00      0.00         0\n",
      "         E12       0.17      1.00      0.29         1\n",
      "         E14       0.00      0.00      0.00         0\n",
      "         E21       0.00      0.00      0.00         0\n",
      "         E31       0.00      0.00      0.00         0\n",
      "         E51       0.00      0.00      0.00         2\n",
      "        E512       0.00      0.00      0.00         0\n",
      "         G15       0.00      0.00      0.00         0\n",
      "        G152       0.00      0.00      0.00         0\n",
      "        GCAT       0.37      0.58      0.45        12\n",
      "       GCRIM       0.00      0.00      0.00         0\n",
      "         M11       0.61      0.74      0.67        27\n",
      "         M12       1.00      1.00      1.00         1\n",
      "         M14       0.17      0.50      0.25         2\n",
      "\n",
      "    accuracy                           0.52      1051\n",
      "   macro avg       0.18      0.29      0.20      1051\n",
      "weighted avg       0.66      0.52      0.57      1051\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0        C18\n",
       "1        C11\n",
       "2        C17\n",
       "3        C31\n",
       "4        E12\n",
       "        ... \n",
       "5248     M11\n",
       "5249     C14\n",
       "5250     C15\n",
       "5251     C33\n",
       "5252    C152\n",
       "Name: labels, Length: 5253, dtype: object"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "applyClassifier(enhancedDFList) # Comparing performance after using autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DEEP NEURAL NETWORKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep Neural Network using Enhanced Features\n",
      "Train on 3532 samples, validate on 883 samples\n",
      "Epoch 1/10\n",
      "3532/3532 [==============================] - 1s 243us/step - loss: 3.7018 - accuracy: 0.1778 - val_loss: 3.9589 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/10\n",
      "3532/3532 [==============================] - 0s 51us/step - loss: 3.0380 - accuracy: 0.2766 - val_loss: 3.8696 - val_accuracy: 0.0091\n",
      "Epoch 3/10\n",
      "3532/3532 [==============================] - 0s 54us/step - loss: 2.9000 - accuracy: 0.1826 - val_loss: 3.7678 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/10\n",
      "3532/3532 [==============================] - 0s 48us/step - loss: 2.8535 - accuracy: 0.2455 - val_loss: 3.9799 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/10\n",
      "3532/3532 [==============================] - 0s 51us/step - loss: 2.8159 - accuracy: 0.2851 - val_loss: 3.8945 - val_accuracy: 0.0000e+00\n",
      "Epoch 6/10\n",
      "3532/3532 [==============================] - 0s 52us/step - loss: 2.7626 - accuracy: 0.2794 - val_loss: 3.8496 - val_accuracy: 0.0000e+00\n",
      "Epoch 7/10\n",
      "3532/3532 [==============================] - 0s 51us/step - loss: 2.7584 - accuracy: 0.2721 - val_loss: 3.9780 - val_accuracy: 0.0000e+00\n",
      "Epoch 8/10\n",
      "3532/3532 [==============================] - 0s 52us/step - loss: 2.7499 - accuracy: 0.2783 - val_loss: 3.8764 - val_accuracy: 0.0000e+00\n",
      "Epoch 9/10\n",
      "3532/3532 [==============================] - 0s 51us/step - loss: 2.7268 - accuracy: 0.2845 - val_loss: 3.8950 - val_accuracy: 0.0000e+00\n",
      "Epoch 10/10\n",
      "3532/3532 [==============================] - 0s 51us/step - loss: 2.7085 - accuracy: 0.2854 - val_loss: 3.9343 - val_accuracy: 0.0000e+00\n",
      "3532/3532 [==============================] - 0s 63us/step\n",
      "accuracy for Deep Neural Networks is\n",
      "0.287089467048645\n",
      "Loss in Deep Neural Networks is\n",
      "2.709868862026815\n"
     ]
    }
   ],
   "source": [
    "# Major Differences\n",
    "# Deep Neural Network using 3 layers is implemented and moreover the feature extraction is enhanced using auto encoder\n",
    "# Doc2Vec is used for vectorization of documents apart from assignment one as TF-IDF generates sparse matrix which is ineffic\n",
    "# Neural networks with 3 hidden layers are used to enhance the performance.\n",
    "def deepNeuralNet(rXtr, rXte, rYtr, rYte):\n",
    "    print(\"Deep Neural Network using Enhanced Features\")\n",
    "    le = LabelEncoder()\n",
    "    rYtr = le.fit_transform(rYtr)\n",
    "    rYte = le.fit_transform(rYte)\n",
    "    dup = numpy.unique(rYtr)\n",
    "    classifier = Sequential()\n",
    "    # First Hidden Layer\n",
    "    classifier.add(Dense(512, activation='relu', input_dim=rXtr.shape[1]))\n",
    "    classifier.add(Dropout(0.5))\n",
    "    # Second  Hidden Layer\n",
    "    classifier.add(Dense(512, activation='relu', input_dim=512))\n",
    "    classifier.add(Dropout(0.5))\n",
    "    classifier.add(Dense(512, activation='relu', input_dim=512))\n",
    "    classifier.add(Dropout(0.5))\n",
    "    classifier.add(Dense(512, activation='relu', input_dim=512))\n",
    "    classifier.add(Dropout(0.5))\n",
    "    # Output Layer\n",
    "    classifier.add(Dense(dup.size, activation='softmax'))\n",
    "    classifier.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    classifier.fit(rXtr, rYtr,validation_data=(rXte, rYte), batch_size=1024, epochs=10)\n",
    "    loss,accuracy = classifier.evaluate(rXtr,rYtr)\n",
    "    print(\"accuracy for Deep Neural Networks is\")\n",
    "    print(accuracy)\n",
    "    print(\"Loss in Deep Neural Networks is\")\n",
    "    print(loss)\n",
    "    \n",
    "def trainTestSplit(dataFrame):\n",
    "    targetF = dataFrame['labels']\n",
    "    splitDFF = dataFrame.iloc[:,:-1]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(splitDFF, targetF, test_size=0.2, random_state=101)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "    \n",
    "Xtr, Xte, Ytr, Yte = trainTestSplit(enhancedDFList[0])\n",
    "deepNeuralNet(Xtr, Xte, Ytr, Yte)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
